[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Блог посвящен вопросам машинного обучения и связанным с ним вопросам"
  },
  {
    "objectID": "posts/welcome/PUG Dataset.html",
    "href": "posts/welcome/PUG Dataset.html",
    "title": "Найдено применение фотореализму UE5",
    "section": "",
    "text": "Найдено применение фотореализму UE5\n\nИ помог в этом PUG (Photorealistic Unreal Graphics). Так как никто не любит проблемы с авторскими данными, но всем хочется получить для себя немного фотореалистичных изображений для бенчмаркинга и оценки моделей (а с фотореалистичной сгенерированной синтетикой есть проблемы), то энтузиасты (с помощью Unreal Engine 5) этого фотореализма и добились, создав 4 датасета и все нужные инструкции к их использованию, включая 3D модели, использованные для генерации. Конечно, они с нами поделились.\nПочитать: \nРепозиторий: \nВ данном блоге буду резмещать посты касающиеся машинного обучения и связанных с ним вопросов"
  },
  {
    "objectID": "posts/post-with-code/HowToUseCopyCode.html",
    "href": "posts/post-with-code/HowToUseCopyCode.html",
    "title": "Aналог Github Copilot на Stability AI",
    "section": "",
    "text": "Stability AI (авторы StableDiffusion) анонсировали аналог Github Copilot — StableCode .\nНейросеть доступна в трёх вариантах: базовый, модель для инструкций и расширенная с контекстным окном до 16 тысячи токенов. Из языков программирования нейросеть поддерживает такие как Python, Java, JavaScript, Go, C, C++.\nМодели же доступны на Huggin Face:\n— stablecode-completion-alpha-3b-4k\n— stablecode-completion-alpha-3b\n— stablecode-instruct-alpha-3b\nРассмотрим возможность использования данной модели\nУстановим transformers\n\n\nCode\n#!pip install transformers==4.29.2\n!pip install transformers==4.31.0\n\n\nCollecting transformers==4.31.0\n  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.4/7.4 MB 16.8 MB/s eta 0:00:00\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (3.12.2)\nCollecting huggingface-hub&lt;1.0,&gt;=0.14.1 (from transformers==4.31.0)\n  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 28.4 MB/s eta 0:00:00\nRequirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (1.23.5)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (23.1)\nRequirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2022.10.31)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2.31.0)\nCollecting tokenizers!=0.11.3,&lt;0.14,&gt;=0.11.1 (from transformers==4.31.0)\n  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 40.5 MB/s eta 0:00:00\nCollecting safetensors&gt;=0.3.1 (from transformers==4.31.0)\n  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 45.2 MB/s eta 0:00:00\nRequirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (4.65.0)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&lt;1.0,&gt;=0.14.1-&gt;transformers==4.31.0) (2023.6.0)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&lt;1.0,&gt;=0.14.1-&gt;transformers==4.31.0) (4.7.1)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers==4.31.0) (3.2.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers==4.31.0) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers==4.31.0) (1.26.16)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers==4.31.0) (2023.7.22)\nInstalling collected packages: tokenizers, safetensors, huggingface-hub, transformers\nSuccessfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n\n\nУстановим инструмент для работы с Haggingface\n\n\nCode\n!pip install huggingface_hub\n\n\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.16.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.12.2)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\nRequirement already satisfied: tqdm&gt;=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.65.0)\nRequirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.7.1)\nRequirement already satisfied: packaging&gt;=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.1)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface_hub) (3.2.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface_hub) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface_hub) (1.26.16)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface_hub) (2023.7.22)\n\n\nДля получения доступа надо сформировать токен на Haggingface\n\nСохраним токен\n\n\nCode\n!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('&lt;Ваш токен&gt;')\"\n\n\nЗарегистрируемся в Haggingface с вводом токена\n\n\nCode\nfrom huggingface_hub import notebook_login\nnotebook_login()\n\n\n\n\n\n\n\nCode\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"stabilityai/stablecode-instruct-alpha-3b\", use_auth_token=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n  \"stabilityai/stablecode-instruct-alpha-3b\",\n  trust_remote_code=True,\n  torch_dtype=\"auto\",\n  use_auth_token=True,\n)\n\n\n\n\n\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# @title Load question  { display-mode: \"form\" }\nmodel.cuda()\nquestion = \"Generate a python function to find number of CPU cores\"# @param {type:\"string\"}\ninputs = tokenizer(\"###Instruction\\n\"+question+\"\\n###Response\\n\",\n                   return_tensors=\"pt\",\n                   return_token_type_ids=False).to(\"cuda\")\n\ntokens = model.generate(\n  **inputs,\n  max_new_tokens=500,\n  temperature=0.2,\n  do_sample=True,\n)\n\nanswer = tokenizer.decode(tokens[0], skip_special_tokens=True)\nprint(answer)\n\n\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n\n\n###Instruction\nGenerate a python function to find number of CPU cores###Response\ndef get_num_cores():\n    \"\"\"\n    This function will return the number of CPU cores in the system\n    \"\"\"\n    # Use the Linux command \"lscpu\" to get the output\n    output = os.popen(\"lscpu\").read()\n    # Split the output into lines\n    lines = output.split(\"\\n\")\n    # Get the number of CPU cores\n    num_cores = len(lines) - 1\n    # Return the number of CPU cores\n    return num_cores\n\n\nПроверим сгенеренный код\n\n\nCode\ndef get_cpu_count():\n    \"\"\"\n    This function will return the number of CPU cores\n    installed in the system\n    \"\"\"\n    import multiprocessing\n    return multiprocessing.cpu_count()\n\n\n\n\nCode\nget_cpu_count()\n\n\n2\n\n\nСкачивать постоянно модель не совсем удобно. По этому рассмотрим сохранение модели и работу с сихраненной моделью\nсоздадим папку для хранения модели\n\n\nCode\n!mkdir model_files\n\n\nNotImplementedError: ignored\n\n\n\n\nCode\n# Save huggingface model files to working directorty\nmodel.save_pretrained('model_files')\ntokenizer.save_pretrained('model_files')\n\n\n('model_files/tokenizer_config.json',\n 'model_files/special_tokens_map.json',\n 'model_files/tokenizer.json')\n\n\n\n\nCode\ntokenizer1 = AutoTokenizer.from_pretrained(\"stabilityai/stablecode-instruct-alpha-3b\",\n                                           use_auth_token=True,\n                                           cache_dir = 'model_files')\nmodel1 = AutoModelForCausalLM.from_pretrained(\n  \"stabilityai/stablecode-instruct-alpha-3b\",\n  trust_remote_code=True,\n  torch_dtype=\"auto\",\n  use_auth_token=True,\n  cache_dir = 'model_files',\n)\n\n\n\n\n\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nmodel1.cuda()\ninputs = tokenizer1(\"###Instruction\\nGenerate a python function to find number of CPU cores###Response\\n\",\n                   return_tensors=\"pt\",\n                   return_token_type_ids=False).to(\"cuda\")\ntokens = model1.generate(\n  **inputs,\n  max_new_tokens=500,\n  temperature=0.2,\n  do_sample=True,\n)\nprint(tokenizer.decode(tokens[0], skip_special_tokens=True))\n\n\n/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n\n\n###Instruction\nGenerate a python function to find number of CPU cores###Response\ndef get_num_cores():\n    \"\"\"\n    This function returns the number of CPU cores in the system\n    \"\"\"\n    # Use the cpuinfo library to get the number of cores\n    from cpuinfo import cpuinfo\n    info = cpuinfo.get_cpu_info()\n    num_cores = info['count']\n    return num_cores\n\n\nИ проверим код сгенеренный\n\n\nCode\ndef get_num_cores():\n    \"\"\"\n    This function returns the number of CPU cores in the system\n    \"\"\"\n    # Use the cpuinfo library to get the number of cores\n    from cpuinfo import cpuinfo\n    info = cpuinfo.get_cpu_info()\n    num_cores = info['count']\n    return num_cores\n\nget_num_cores()\n\n\n2"
  },
  {
    "objectID": "posts/post-with-code/SDXL_Demo.html",
    "href": "posts/post-with-code/SDXL_Demo.html",
    "title": "Генерация изображений с помощью модели SDXL (sd_xl_base_1.0)",
    "section": "",
    "text": "Тут мы решили посмотреть насколько лучше стало качество генерации у новой модели SDXL (sd_xl_base_1.0) по сравнению с SD2. Конечно вариантов Stable Diffusion моделей уже есть большое множество, и каждая затюнина под своё, но мы специально брали стоковую для сравнения.\nУстановим diffusers версии &gt;= 0.19.0:\n\n\nCode\n!pip install diffusers --upgrade\n\n\nCollecting diffusers\n  Downloading diffusers-0.20.0-py3-none-any.whl (1.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 8.7 MB/s eta 0:00:00\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (6.8.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.12.2)\nCollecting huggingface-hub&gt;=0.13.2 (from diffusers)\n  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 11.1 MB/s eta 0:00:00\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.23.5)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2023.6.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.31.0)\nCollecting safetensors&gt;=0.3.1 (from diffusers)\n  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 15.2 MB/s eta 0:00:00\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (9.4.0)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&gt;=0.13.2-&gt;diffusers) (2023.6.0)\nRequirement already satisfied: tqdm&gt;=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&gt;=0.13.2-&gt;diffusers) (4.66.1)\nRequirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&gt;=0.13.2-&gt;diffusers) (6.0.1)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&gt;=0.13.2-&gt;diffusers) (4.7.1)\nRequirement already satisfied: packaging&gt;=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&gt;=0.13.2-&gt;diffusers) (23.1)\nRequirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata-&gt;diffusers) (3.16.2)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;diffusers) (3.2.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;diffusers) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;diffusers) (2.0.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;diffusers) (2023.7.22)\nInstalling collected packages: safetensors, huggingface-hub, diffusers\nSuccessfully installed diffusers-0.20.0 huggingface-hub-0.16.4 safetensors-0.3.2\n\n\nКроме того, обязательно установите transformers, safetensors, accelerate, а также invisible watermark:\n\n\nCode\n!pip install invisible_watermark transformers accelerate safetensors\n\n\nCollecting invisible_watermark\n  Downloading invisible_watermark-0.2.0-py3-none-any.whl (1.6 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 8.5 MB/s eta 0:00:00\nCollecting transformers\n  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.4/7.4 MB 26.1 MB/s eta 0:00:00\nCollecting accelerate\n  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 244.2/244.2 kB 19.2 MB/s eta 0:00:00\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (0.3.2)\nRequirement already satisfied: Pillow&gt;=6.0.0 in /usr/local/lib/python3.10/dist-packages (from invisible_watermark) (9.4.0)\nRequirement already satisfied: PyWavelets&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from invisible_watermark) (1.4.1)\nRequirement already satisfied: numpy&gt;=1.17.0 in /usr/local/lib/python3.10/dist-packages (from invisible_watermark) (1.23.5)\nRequirement already satisfied: opencv-python&gt;=4.1.0.25 in /usr/local/lib/python3.10/dist-packages (from invisible_watermark) (4.8.0.76)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from invisible_watermark) (2.0.1+cu118)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub&lt;1.0,&gt;=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\nRequirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\nCollecting tokenizers!=0.11.3,&lt;0.14,&gt;=0.11.1 (from transformers)\n  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 51.3 MB/s eta 0:00:00\nRequirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&lt;1.0,&gt;=0.14.1-&gt;transformers) (2023.6.0)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&lt;1.0,&gt;=0.14.1-&gt;transformers) (4.7.1)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch-&gt;invisible_watermark) (1.12)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch-&gt;invisible_watermark) (3.1)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-&gt;invisible_watermark) (3.1.2)\nRequirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch-&gt;invisible_watermark) (2.0.0)\nRequirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-&gt;torch-&gt;invisible_watermark) (3.27.2)\nRequirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-&gt;torch-&gt;invisible_watermark) (16.0.6)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (3.2.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (2.0.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (2023.7.22)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;torch-&gt;invisible_watermark) (2.1.3)\nRequirement already satisfied: mpmath&gt;=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-&gt;torch-&gt;invisible_watermark) (1.3.0)\nInstalling collected packages: tokenizers, transformers, invisible_watermark, accelerate\nSuccessfully installed accelerate-0.21.0 invisible_watermark-0.2.0 tokenizers-0.13.3 transformers-4.31.0\n\n\nЧтобы просто использовать базовую модель, вы можете запустить:\n\n\nCode\nimport torch\nfrom diffusers import DiffusionPipeline\n\n\n\n\npipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\",\n                                         torch_dtype=torch.float16,\n                                         use_safetensors=True,\n                                         variant=\"fp16\")\npipe.to(\"cuda\")\n# If you are limited by GPU VRAM, you can enable cpu offloading by calling pipe.enable_model_cpu_offload instead of .to(\"cuda\"):\n#pipe.enable_model_cpu_offload()\n\n\n#When using torch &gt;= 2.0, you can improve the inference speed by 20-30% with torch.compile.\n#torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n\n# if using torch &lt; 2.0\n# pipe.enable_xformers_memory_efficient_attention()\n\nprompt = \"A majestic lion jumping from a big stone at night\"\n\nimages = pipe(prompt=prompt).images[0]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimages\n\n\n\n\n\nДобавим негативный промпт, количество шагов, sampler, назмер выходного изображения\n\n\nCode\nneg_prompt =\"text, watermark,cartoon, anime\"\nsteps=100\nsampler=\"euler\"\n\nimages = pipe(prompt=prompt,\n              negative_prompt=neg_prompt,\n              num_inference_steps=steps,height=512, width=512).images[0]\nimages\n\n\n\n\n\n\n\n\nДобавим генерацию трех картинок и их вывод\n\n\nCode\nfrom PIL import Image\n\ndef image_grid(imgs, rows, cols):\n    assert len(imgs) == rows*cols\n\n    w, h = imgs[0].size\n    grid = Image.new('RGB', size=(cols*w, rows*h))\n    grid_w, grid_h = grid.size\n\n    for i, img in enumerate(imgs):\n        grid.paste(img, box=(i%cols*w, i//cols*h))\n    return grid\n\n\n\n\nCode\ngenerator = torch.Generator(\"cuda\").manual_seed(42)\nimages_num = 3\nheight = 512\nwidth = 512\n\nimages = pipe(prompt, guidance_scale=7.5, generator=generator,negative_prompt=neg_prompt,\n              num_inference_steps=steps,height=height, width=width,num_images_per_prompt=images_num)\ngrid = image_grid(images[0], rows=1, cols=images_num)\ngrid\n\n\n\n\n\n\n\n\nЧтобы использовать весь базовый конвейер + уточнение как ансамбль экспертов  , вы можете запустить:\n\n\nCode\nfrom diffusers import DiffusionPipeline\nimport torch\n\n# load both base & refiner\npipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\",\n                                         torch_dtype=torch.float16,\n                                         variant=\"fp16\",\n                                         use_safetensors=True\n)\n\n#pipe.to(\"cuda\")\npipe.enable_model_cpu_offload() #у меня не хватает GPU RAM\n\nrefiner = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n    text_encoder_2=pipe.text_encoder_2,\n    vae=pipe.vae,\n    torch_dtype=torch.float16,\n    use_safetensors=True,\n    variant=\"fp16\",\n)\nrefiner.to(\"cuda\")\n\n# Define how many steps and what % of steps to be run on each experts (80/20) here\nn_steps = 40\nhigh_noise_frac = 0.8\n\nprompt = \"A majestic lion jumping from a big stone at night\"\ngenerator = torch.Generator(\"cuda\").manual_seed(42)\nheight = 512\nwidth = 512\nneg_prompt =\"text, watermark,cartoon, anime\"\nsteps=100\nsampler=\"euler\"\n\n# run both experts\nimage = pipe(\n    prompt=prompt,\n    num_inference_steps=n_steps,\n    denoising_end=high_noise_frac,\n    generator=generator,\n    negative_prompt=neg_prompt,\n    height=height,\n    width=width,\n    output_type=\"latent\",\n).images\nimage = refiner(\n    prompt=prompt,\n    num_inference_steps=n_steps,\n    denoising_start=high_noise_frac,\n    image=image,\n).images[0]\n\nimage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt seems like you have activated model offloading by calling `enable_model_cpu_offload`, but are now manually moving the pipeline to GPU. It is strongly recommended against doing so as memory gains from offloading are likely to be lost. Offloading automatically takes care of moving the individual components vae, text_encoder, text_encoder_2, tokenizer, tokenizer_2, unet, scheduler to GPU when needed. To make sure offloading works as expected, you should consider moving the pipeline back to CPU: `pipeline.to('cpu')` or removing the move altogether if you use offloading."
  },
  {
    "objectID": "posts/post-with-code/rnn_mario_level_builder.html",
    "href": "posts/post-with-code/rnn_mario_level_builder.html",
    "title": "Create Mario Levels",
    "section": "",
    "text": "Рассмотрим применение RNN для создания игровых уровней\nпо этому адресу размещены карты уровней Супер Марио.\nВозьмем от-туда карту\n\nОткроем ее в gimp. Включим сетку, и текстом прокодируем спрайты\n\n\n\nimage.png\n\n\nВ результате получаем текстовый файл описывающий препятствия\n\n\n\nimage.png\n\n\nСохраним его, и приступим к написанию кода\nСначала мы загрузим данные уровня в переменную Level\n\n\nCode\nimport numpy as np\n\nfname = '/content/drive/MyDrive/Проекты/ML/Pets/rnn_mario_level_builder/data/MarioL1_.txt'\n#np.loadtxt(fname, dtype=’float’, comments=’#’, delimiter=None, converters=None, skiprows=0, usecols=None, unpack=False, ndmin=0 encoding='bytes', max_rows=None)\n\nLevel = np.loadtxt(fname, dtype='str',comments=None) \nprint(Level)\n\n\n['--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------'\n '--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------'\n '--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------'\n '------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------'\n '------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------'\n '----------------------?---------------------------------------------------------========---===?--------------?-----------===----=??=--------------------------------------------------------##--------|-------------------------'\n '-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------###--------|-------------------------'\n '------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------####--------|-------------------------'\n '-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------#####--------|----\\\\\\\\\\\\------------------'\n '----------------?---=-=?=---------------------PP---------PP------------------=?=--------------=-----==----?--?--?-----=----------==------#--#----------##--#------------==?=------------######--------|----CSD------------------'\n '--------------------------------------PP------PP---------PP-----------------------------------------------------------------------------##--##--------###--##--------------------------#######--------|---\\\\\\\\\\\\\\\\\\\\-----------------'\n '----------------------------PP--------PP------PP---------PP----------------------------------------------------------------------------###--###------####--###-----PP--------------PP-########--------|---SSUSS-----------------'\n '----------------------------PP--------PP------PP---------PP---------------------------------------------------------------------------####--####----#####--####----PP--------------PP#########--------#---SSOSS-----------------'\n 'GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG--GGGGGGGGGGGGGGG---GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG--GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG'\n 'GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG--GGGGGGGGGGGGGGG---GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG--GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG']\n\n\nДанные не годятся для обучения, так как у нас всего 15 строк и 255 признаков. Если повернем набор, то у нас будет 15 признаков и 255 строк, что уже лучше.\n\n\nCode\ncol=len(Level[0])\nrow=len(Level)\nLevel.shape\n\n\nLevelP=np.zeros((row,col), dtype='S1')\n\nprint(LevelP.shape)\n\nfor i in range(row):\n  \n  for j in range(col):\n    \n    LevelP[i,j]=Level[i][col-j-1]\n\nwith np.printoptions(threshold=np.inf, linewidth=1275):\n  print(LevelP)\n\n\n(15, 224)\n[[b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-']\n [b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-']\n [b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-']\n [b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'|' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-']\n [b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'|' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-']\n [b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'|' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'#' b'#' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'=' b'?' b'?' b'=' b'-' b'-' b'-' b'-' b'=' b'=' b'=' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'?' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'?' b'=' b'=' b'=' b'-' b'-' b'-' b'=' b'=' b'=' b'=' b'=' b'=' b'=' b'=' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'?' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-']\n [b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'|' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'#' b'#' b'#' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-']\n [b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'|' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'#' b'#' b'#' b'#' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-']\n [b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'\\\\' b'\\\\' b'\\\\' b'-' b'-' b'-' b'-' b'|' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'#' b'#' b'#' b'#' b'#' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-']\n [b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'D' b'S' b'C' b'-' b'-' b'-' b'-' b'|' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'#' b'#' b'#' b'#' b'#' b'#' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'=' b'?' b'=' b'=' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'#' b'-' b'-' b'#' b'#' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'#' b'-' b'-' b'#' b'-' b'-' b'-' b'-' b'-' b'-' b'=' b'=' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'=' b'-' b'-' b'-' b'-' b'-' b'?' b'-' b'-' b'?' b'-' b'-' b'?' b'-' b'-' b'-' b'-' b'=' b'=' b'-' b'-' b'-' b'-' b'-' b'=' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'=' b'?' b'=' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'P' b'P' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'P' b'P' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'=' b'?' b'=' b'-' b'=' b'-' b'-' b'-' b'?' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-']\n [b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'\\\\' b'\\\\' b'\\\\' b'\\\\' b'\\\\' b'-' b'-' b'-' b'|' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'#' b'#' b'#' b'#' b'#' b'#' b'#' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'#' b'#' b'-' b'-' b'#' b'#' b'#' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'#' b'#' b'-' b'-' b'#' b'#' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'P' b'P' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'P' b'P' b'-' b'-' b'-' b'-' b'-' b'-' b'P' b'P' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-']\n [b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'S' b'S' b'U' b'S' b'S' b'-' b'-' b'-' b'|' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'#' b'#' b'#' b'#' b'#' b'#' b'#' b'#' b'-' b'P' b'P' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'P' b'P' b'-' b'-' b'-' b'-' b'-' b'#' b'#' b'#' b'-' b'-' b'#' b'#' b'#' b'#' b'-' b'-' b'-' b'-' b'-' b'-' b'#' b'#' b'#' b'-' b'-' b'#' b'#' b'#' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'P' b'P' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'P' b'P' b'-' b'-' b'-' b'-' b'-' b'-' b'P' b'P' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'P' b'P' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-']\n [b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'S' b'S' b'O' b'S' b'S' b'-' b'-' b'-' b'#' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'#' b'#' b'#' b'#' b'#' b'#' b'#' b'#' b'#' b'P' b'P' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'P' b'P' b'-' b'-' b'-' b'-' b'#' b'#' b'#' b'#' b'-' b'-' b'#' b'#' b'#' b'#' b'#' b'-' b'-' b'-' b'-' b'#' b'#' b'#' b'#' b'-' b'-' b'#' b'#' b'#' b'#' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'P' b'P' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'P' b'P' b'-' b'-' b'-' b'-' b'-' b'-' b'P' b'P' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'P' b'P' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-']\n [b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'-' b'-' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'-' b'-' b'-' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'-' b'-' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G']\n [b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'-' b'-' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'-' b'-' b'-' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'-' b'-' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G' b'G']]\n\n\n\n\nCode\ncoln, rown=LevelP.shape\nprint(f\"row:{rown} col:{coln}\")\n\n\nrow:224 col:15\n\n\n\n\nCode\nПриведем данные массива к строковому виду\n\n\n\n\nCode\nraw_text=\"\"\n\nfor i in range(rown):\n  for j in range(coln):\n    raw_text=raw_text+LevelP[j,rown-i-1].decode('UTF-8')\n  raw_text=raw_text+'\\n'\n\nprint(raw_text)\n\n\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n---------?---GG\n-------------GG\n-------------GG\n-------------GG\n---------=---GG\n-------------GG\n-----?---=---GG\n---------?---GG\n---------=---GG\n-------------GG\n-------------GG\n-------------GG\n-----------PPGG\n-----------PPGG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n----------PPPGG\n----------PPPGG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n---------PPPPGG\n---------PPPPGG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n---------PPPPGG\n---------PPPPGG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n---------------\n---------------\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n---------=---GG\n---------?---GG\n---------=---GG\n-----=-------GG\n-----=-------GG\n-----=-------GG\n-----=-------GG\n-----=-------GG\n-----=-------GG\n-----=---------\n-----=---------\n---------------\n-------------GG\n-------------GG\n-----=-------GG\n-----=-------GG\n-----=-------GG\n-----?---=---GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n---------=---GG\n---------=---GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n---------?---GG\n-------------GG\n-------------GG\n-----?---?---GG\n-------------GG\n-------------GG\n---------?---GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n---------=---GG\n-------------GG\n-------------GG\n-----=-------GG\n-----=-------GG\n-----=-------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-----=-------GG\n-----?---=---GG\n-----?---=---GG\n-----=-------GG\n-------------GG\n-------------GG\n------------#GG\n-----------##GG\n----------###GG\n---------####GG\n-------------GG\n-------------GG\n---------####GG\n----------###GG\n-----------##GG\n------------#GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n------------#GG\n-----------##GG\n----------###GG\n---------####GG\n---------####GG\n---------------\n---------------\n---------####GG\n----------###GG\n-----------##GG\n------------#GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-----------PPGG\n-----------PPGG\n-------------GG\n-------------GG\n-------------GG\n---------=---GG\n---------=---GG\n---------?---GG\n---------=---GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-----------PPGG\n-----------PPGG\n------------#GG\n-----------##GG\n----------###GG\n---------####GG\n--------#####GG\n-------######GG\n------#######GG\n-----########GG\n-----########GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n---|||||||||#GG\n-------------GG\n-------------GG\n-------------GG\n----------\\SSGG\n--------\\C\\SSGG\n--------\\S\\UOGG\n--------\\D\\SSGG\n----------\\SSGG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n\n\n\nТеперь можно приступать к сборке и обучению модели\n\n\nCode\n# create mapping of unique chars to integers\nchars = sorted(list(set(raw_text)))\nchar_to_int = dict((c, i) for i, c in enumerate(chars))\n\n\n\n\nCode\nn_chars = len(raw_text)\nn_vocab = len(chars)\nprint (f\"Total Characters: {n_chars}\")\nprint (f\"Total Vocab: {n_vocab}\")\n\n\nTotal Characters: 3584\nTotal Vocab: 14\n\n\nМы видим, что набор содержит более 3000 символов и что при преобразовании в словаре есть только 14 различных символов для изучения сетью. Гораздо меньше, чем 32 в алфавите.\nТеперь нам нужно определить данные обучения для сети. Есть множество вариантов по разбиению текста и подаче его в сеть во время обучения.\nНа этом этапе мы разделим текст на подпоследовательности с фиксированной длиной в 160 символов.\nКаждый обучающий шаблон сети состоит из 160 временных шагов одного символа (X), за которыми следует один символьный вывод (y). При создании этих последовательностей мы перемещаем это окно по всей книге по одному символу за раз, позволяя каждому символу получить шанс из 160 предшествующих ему символов (кроме, конечно, первых 160 символов).\n\n\nCode\n# prepare the dataset of input to output pairs encoded as integers\nseq_length = 160\ndataX = []\ndataY = []\n\nfor i in range(0, n_chars - seq_length, 1):\n    seq_in = raw_text[i:i + seq_length]\n    seq_out = raw_text[i + seq_length]\n    dataX.append([char_to_int[char] for char in seq_in])\n    dataY.append(char_to_int[seq_out])\nn_patterns = len(dataX)\n\nprint (f\"Total Patterns: {n_patterns}\")\n\n\nTotal Patterns: 3424\n\n\nТеперь, когда мы подготовили наши тренировочные данные, нам нужно преобразовать их так, чтобы они подходили для использования с Keras.\nСначала мы должны преобразовать список входных последовательностей в форму[образцы, временные шаги, особенности]ожидается сетью LSTM.\nЗатем нам нужно изменить масштаб целых чисел в диапазоне от 0 до 1, чтобы облегчить изучение шаблонов сетью LSTM, которая по умолчанию использует функцию активации сигмовидной кишки.\nНаконец, нам нужно преобразовать выходные шаблоны (отдельные символы, преобразованные в целые числа) в одну горячую кодировку. Это сделано для того, чтобы мы могли настроить сеть так, чтобы она предсказывала вероятность каждого из 14 различных символов в словаре (более простое представление), а не пыталась заставить ее предсказать точно следующий символ. Каждое значение y преобразуется в разреженный вектор длиной 14, полный нулей, за исключением 1 в столбце для буквы (целое число), которую представляет шаблон.\nМы можем реализовать эти шаги, как показано ниже.\n\n\nCode\nfrom keras.utils import np_utils\n\n# reshape X to be [samples, time steps, features]\nX = np.reshape(dataX, (n_patterns, seq_length, 1))\n# normalize\nX = X / float(n_vocab)\n# one hot encode the output variable\ny = np_utils.to_categorical(dataY)\n\n\n\n\nCode\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import LSTM\nfrom keras.callbacks import ModelCheckpoint\n\n\n\n\nCode\ny.shape[1]\n\n\n14\n\n\nТеперь мы можем определить нашу модель LSTM. Здесь мы определяем один скрытый слой LSTM с 256 единицами памяти. Сеть использует выпадение с вероятностью 20. Выходной уровень - это Плотный уровень, использующий функцию активации softmax для вывода прогнозирования вероятности для каждого из 14 символов в диапазоне от 0 до 1.\nЭта проблема на самом деле представляет собой проблему классификации отдельных символов с 14 классами, и поэтому она определяется как оптимизация потерь в журнале (перекрестная энтропия) с использованием алгоритма оптимизации ADAM по скорости.\n\n\nCode\n# define the LSTM model\nmodel = Sequential()\nmodel.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(y.shape[1], activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n\nТестового набора данных нет. Мы моделируем весь обучающий набор данных, чтобы узнать вероятность каждого персонажа в последовательности.\nНас не интересует наиболее точная (точность классификации) модель учебного набора данных. Это будет модель, которая идеально предсказывает каждого символа в наборе обучающих данных. Вместо этого мы заинтересованы в обобщении набора данных, который минимизирует выбранную функцию потерь. Мы ищем баланс между обобщением и переобучением, но без запоминания.\nСеть работает медленно (около 300 секунд на эпоху на графическом процессоре Nvidia K520). Из-за медлительности и из-за наших требований по оптимизации мы будем использовать контрольные точки модели для записи всех сетевых весов, чтобы каждый раз регистрировать улучшение потерь в конце эпохи. Мы будем использовать лучший набор весов (наименьшая потеря), чтобы реализовать нашу генеративную модель в следующем разделе.\n\n\nCode\n# define the checkpoint\nfilepath=\"/content/drive/MyDrive/Проекты/ML/Pets/rnn_mario_level_builder/data/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\ncallbacks_list = [checkpoint]\n\n\nТеперь мы можем приспособить нашу модель к данным. Здесь мы используем скромное количество из 100 эпох и большой размер пакета из 128 шаблонов.\n\n\nCode\nhistory=model.fit(X, y, epochs=1000, batch_size=128, callbacks=callbacks_list)\n\n\nEpoch 1/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0184\nEpoch 1: loss did not improve from 0.00715\n27/27 [==============================] - 2s 25ms/step - loss: 0.0184\nEpoch 2/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0220\nEpoch 2: loss did not improve from 0.00715\n27/27 [==============================] - 1s 20ms/step - loss: 0.0221\nEpoch 3/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0113\nEpoch 3: loss did not improve from 0.00715\n27/27 [==============================] - 1s 20ms/step - loss: 0.0116\nEpoch 4/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0127\nEpoch 4: loss did not improve from 0.00715\n27/27 [==============================] - 1s 20ms/step - loss: 0.0120\nEpoch 5/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0108\nEpoch 5: loss did not improve from 0.00715\n27/27 [==============================] - 1s 20ms/step - loss: 0.0103\nEpoch 6/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0123\nEpoch 6: loss did not improve from 0.00715\n27/27 [==============================] - 1s 20ms/step - loss: 0.0118\nEpoch 7/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0139\nEpoch 7: loss did not improve from 0.00715\n27/27 [==============================] - 1s 20ms/step - loss: 0.0138\nEpoch 8/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0143\nEpoch 8: loss did not improve from 0.00715\n27/27 [==============================] - 1s 20ms/step - loss: 0.0147\nEpoch 9/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0170\nEpoch 9: loss did not improve from 0.00715\n27/27 [==============================] - 1s 20ms/step - loss: 0.0164\nEpoch 10/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0125\nEpoch 10: loss did not improve from 0.00715\n27/27 [==============================] - 1s 20ms/step - loss: 0.0125\nEpoch 11/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0120\nEpoch 11: loss did not improve from 0.00715\n27/27 [==============================] - 1s 20ms/step - loss: 0.0124\nEpoch 12/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0117\nEpoch 12: loss did not improve from 0.00715\n27/27 [==============================] - 1s 19ms/step - loss: 0.0133\nEpoch 13/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0190\nEpoch 13: loss did not improve from 0.00715\n27/27 [==============================] - 1s 20ms/step - loss: 0.0182\nEpoch 14/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0140\nEpoch 14: loss did not improve from 0.00715\n27/27 [==============================] - 1s 20ms/step - loss: 0.0167\nEpoch 15/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0295\nEpoch 15: loss did not improve from 0.00715\n27/27 [==============================] - 1s 20ms/step - loss: 0.0278\nEpoch 16/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0225\nEpoch 16: loss did not improve from 0.00715\n27/27 [==============================] - 1s 20ms/step - loss: 0.0228\nEpoch 17/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0195\nEpoch 17: loss did not improve from 0.00715\n27/27 [==============================] - 1s 20ms/step - loss: 0.0192\nEpoch 18/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0188\nEpoch 18: loss did not improve from 0.00715\n27/27 [==============================] - 1s 20ms/step - loss: 0.0203\nEpoch 19/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0135\nEpoch 19: loss did not improve from 0.00715\n27/27 [==============================] - 1s 20ms/step - loss: 0.0128\nEpoch 20/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0250\nEpoch 20: loss did not improve from 0.00715\n27/27 [==============================] - 1s 20ms/step - loss: 0.0236\nEpoch 21/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0162\nEpoch 21: loss did not improve from 0.00715\n27/27 [==============================] - 1s 20ms/step - loss: 0.0162\nEpoch 22/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0138\nEpoch 22: loss did not improve from 0.00715\n27/27 [==============================] - 1s 20ms/step - loss: 0.0137\nEpoch 23/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0118\nEpoch 23: loss did not improve from 0.00715\n27/27 [==============================] - 1s 20ms/step - loss: 0.0122\nEpoch 24/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0139\nEpoch 24: loss did not improve from 0.00715\n27/27 [==============================] - 1s 20ms/step - loss: 0.0131\nEpoch 25/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0106\nEpoch 25: loss did not improve from 0.00715\n27/27 [==============================] - 1s 20ms/step - loss: 0.0101\nEpoch 26/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0096\nEpoch 26: loss did not improve from 0.00715\n27/27 [==============================] - 1s 20ms/step - loss: 0.0091\nEpoch 27/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0079\nEpoch 27: loss did not improve from 0.00715\n27/27 [==============================] - 1s 20ms/step - loss: 0.0075\nEpoch 28/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0087\nEpoch 28: loss did not improve from 0.00715\n27/27 [==============================] - 1s 20ms/step - loss: 0.0082\nEpoch 29/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0091\nEpoch 29: loss did not improve from 0.00715\n27/27 [==============================] - 1s 20ms/step - loss: 0.0088\nEpoch 30/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0085\nEpoch 30: loss did not improve from 0.00715\n27/27 [==============================] - 1s 20ms/step - loss: 0.0081\nEpoch 31/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0082\nEpoch 31: loss did not improve from 0.00715\n27/27 [==============================] - 1s 20ms/step - loss: 0.0081\nEpoch 32/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0074\nEpoch 32: loss improved from 0.00715 to 0.00705, saving model to /content/drive/MyDrive/Проекты/ML/Pets/rnn_mario_level_builder/data/weights-improvement-32-0.0070.hdf5\n27/27 [==============================] - 1s 22ms/step - loss: 0.0070\nEpoch 33/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0077\nEpoch 33: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0074\nEpoch 34/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0073\nEpoch 34: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0111\nEpoch 35/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0127\nEpoch 35: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0123\nEpoch 36/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0118\nEpoch 36: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0113\nEpoch 37/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0079\nEpoch 37: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0096\nEpoch 38/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0091\nEpoch 38: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0086\nEpoch 39/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0079\nEpoch 39: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0076\nEpoch 40/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0105\nEpoch 40: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0102\nEpoch 41/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0161\nEpoch 41: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0153\nEpoch 42/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0189\nEpoch 42: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0181\nEpoch 43/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0198\nEpoch 43: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0209\nEpoch 44/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0390\nEpoch 44: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0392\nEpoch 45/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0277\nEpoch 45: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0272\nEpoch 46/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0199\nEpoch 46: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0192\nEpoch 47/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0151\nEpoch 47: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0144\nEpoch 48/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0127\nEpoch 48: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0136\nEpoch 49/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0141\nEpoch 49: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0164\nEpoch 50/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0184\nEpoch 50: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0175\nEpoch 51/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0166\nEpoch 51: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0169\nEpoch 52/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0118\nEpoch 52: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0114\nEpoch 53/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0093\nEpoch 53: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0090\nEpoch 54/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0089\nEpoch 54: loss did not improve from 0.00705\n27/27 [==============================] - 1s 21ms/step - loss: 0.0087\nEpoch 55/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0135\nEpoch 55: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0185\nEpoch 56/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0325\nEpoch 56: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0336\nEpoch 57/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0435\nEpoch 57: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0441\nEpoch 58/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0248\nEpoch 58: loss did not improve from 0.00705\n27/27 [==============================] - 1s 21ms/step - loss: 0.0236\nEpoch 59/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0133\nEpoch 59: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0147\nEpoch 60/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0135\nEpoch 60: loss did not improve from 0.00705\n27/27 [==============================] - 1s 21ms/step - loss: 0.0129\nEpoch 61/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0162\nEpoch 61: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0159\nEpoch 62/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0165\nEpoch 62: loss did not improve from 0.00705\n27/27 [==============================] - 1s 21ms/step - loss: 0.0170\nEpoch 63/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0114\nEpoch 63: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0131\nEpoch 64/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0107\nEpoch 64: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0114\nEpoch 65/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0077\nEpoch 65: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0087\nEpoch 66/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0119\nEpoch 66: loss did not improve from 0.00705\n27/27 [==============================] - 1s 21ms/step - loss: 0.0116\nEpoch 67/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0165\nEpoch 67: loss did not improve from 0.00705\n27/27 [==============================] - 1s 21ms/step - loss: 0.0164\nEpoch 68/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0164\nEpoch 68: loss did not improve from 0.00705\n27/27 [==============================] - 1s 21ms/step - loss: 0.0165\nEpoch 69/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0224\nEpoch 69: loss did not improve from 0.00705\n27/27 [==============================] - 1s 21ms/step - loss: 0.0213\nEpoch 70/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0267\nEpoch 70: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0337\nEpoch 71/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.1082\nEpoch 71: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.1063\nEpoch 72/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0563\nEpoch 72: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0537\nEpoch 73/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0260\nEpoch 73: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0248\nEpoch 74/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0184\nEpoch 74: loss did not improve from 0.00705\n27/27 [==============================] - 1s 21ms/step - loss: 0.0186\nEpoch 75/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0120\nEpoch 75: loss did not improve from 0.00705\n27/27 [==============================] - 1s 21ms/step - loss: 0.0138\nEpoch 76/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0120\nEpoch 76: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0118\nEpoch 77/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0107\nEpoch 77: loss did not improve from 0.00705\n27/27 [==============================] - 1s 21ms/step - loss: 0.0107\nEpoch 78/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0096\nEpoch 78: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0092\nEpoch 79/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0110\nEpoch 79: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0105\nEpoch 80/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0096\nEpoch 80: loss did not improve from 0.00705\n27/27 [==============================] - 1s 21ms/step - loss: 0.0099\nEpoch 81/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0101\nEpoch 81: loss did not improve from 0.00705\n27/27 [==============================] - 1s 21ms/step - loss: 0.0098\nEpoch 82/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0153\nEpoch 82: loss did not improve from 0.00705\n27/27 [==============================] - 1s 21ms/step - loss: 0.0154\nEpoch 83/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0127\nEpoch 83: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0149\nEpoch 84/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0404\nEpoch 84: loss did not improve from 0.00705\n27/27 [==============================] - 1s 21ms/step - loss: 0.0388\nEpoch 85/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0191\nEpoch 85: loss did not improve from 0.00705\n27/27 [==============================] - 1s 21ms/step - loss: 0.0183\nEpoch 86/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0124\nEpoch 86: loss did not improve from 0.00705\n27/27 [==============================] - 1s 21ms/step - loss: 0.0124\nEpoch 87/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0072\nEpoch 87: loss did not improve from 0.00705\n27/27 [==============================] - 1s 21ms/step - loss: 0.0102\nEpoch 88/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0080\nEpoch 88: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0077\nEpoch 89/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0086\nEpoch 89: loss did not improve from 0.00705\n27/27 [==============================] - 1s 21ms/step - loss: 0.0082\nEpoch 90/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0080\nEpoch 90: loss did not improve from 0.00705\n27/27 [==============================] - 1s 21ms/step - loss: 0.0075\nEpoch 91/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0086\nEpoch 91: loss did not improve from 0.00705\n27/27 [==============================] - 1s 21ms/step - loss: 0.0081\nEpoch 92/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0093\nEpoch 92: loss did not improve from 0.00705\n27/27 [==============================] - 1s 21ms/step - loss: 0.0088\nEpoch 93/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0097\nEpoch 93: loss did not improve from 0.00705\n27/27 [==============================] - 1s 21ms/step - loss: 0.0091\nEpoch 94/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0080\nEpoch 94: loss did not improve from 0.00705\n27/27 [==============================] - 1s 21ms/step - loss: 0.0081\nEpoch 95/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0220\nEpoch 95: loss did not improve from 0.00705\n27/27 [==============================] - 1s 21ms/step - loss: 0.0220\nEpoch 96/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0277\nEpoch 96: loss did not improve from 0.00705\n27/27 [==============================] - 1s 21ms/step - loss: 0.0264\nEpoch 97/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0136\nEpoch 97: loss did not improve from 0.00705\n27/27 [==============================] - 1s 21ms/step - loss: 0.0130\nEpoch 98/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0148\nEpoch 98: loss did not improve from 0.00705\n27/27 [==============================] - 1s 21ms/step - loss: 0.0153\nEpoch 99/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0102\nEpoch 99: loss did not improve from 0.00705\n27/27 [==============================] - 1s 20ms/step - loss: 0.0118\nEpoch 100/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0100\nEpoch 100: loss did not improve from 0.00705\n27/27 [==============================] - 1s 21ms/step - loss: 0.0099\nEpoch 101/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0081\nEpoch 101: loss did not improve from 0.00705\n27/27 [==============================] - 1s 21ms/step - loss: 0.0083\nEpoch 102/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0068\nEpoch 102: loss improved from 0.00705 to 0.00696, saving model to /content/drive/MyDrive/Проекты/ML/Pets/rnn_mario_level_builder/data/weights-improvement-102-0.0070.hdf5\n27/27 [==============================] - 1s 22ms/step - loss: 0.0070\nEpoch 103/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0073\nEpoch 103: loss did not improve from 0.00696\n27/27 [==============================] - 1s 21ms/step - loss: 0.0070\nEpoch 104/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0069\nEpoch 104: loss improved from 0.00696 to 0.00688, saving model to /content/drive/MyDrive/Проекты/ML/Pets/rnn_mario_level_builder/data/weights-improvement-104-0.0069.hdf5\n27/27 [==============================] - 1s 22ms/step - loss: 0.0069\nEpoch 105/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0071\nEpoch 105: loss improved from 0.00688 to 0.00671, saving model to /content/drive/MyDrive/Проекты/ML/Pets/rnn_mario_level_builder/data/weights-improvement-105-0.0067.hdf5\n27/27 [==============================] - 1s 23ms/step - loss: 0.0067\nEpoch 106/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0056\nEpoch 106: loss improved from 0.00671 to 0.00553, saving model to /content/drive/MyDrive/Проекты/ML/Pets/rnn_mario_level_builder/data/weights-improvement-106-0.0055.hdf5\n27/27 [==============================] - 1s 22ms/step - loss: 0.0055\nEpoch 107/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0069\nEpoch 107: loss did not improve from 0.00553\n27/27 [==============================] - 1s 21ms/step - loss: 0.0067\nEpoch 108/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0058\nEpoch 108: loss did not improve from 0.00553\n27/27 [==============================] - 1s 21ms/step - loss: 0.0061\nEpoch 109/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0060\nEpoch 109: loss did not improve from 0.00553\n27/27 [==============================] - 1s 21ms/step - loss: 0.0074\nEpoch 110/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0056\nEpoch 110: loss improved from 0.00553 to 0.00551, saving model to /content/drive/MyDrive/Проекты/ML/Pets/rnn_mario_level_builder/data/weights-improvement-110-0.0055.hdf5\n27/27 [==============================] - 1s 22ms/step - loss: 0.0055\nEpoch 111/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0059\nEpoch 111: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0056\nEpoch 112/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0044\nEpoch 112: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0055\nEpoch 113/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0099\nEpoch 113: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0095\nEpoch 114/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0140\nEpoch 114: loss did not improve from 0.00551\n27/27 [==============================] - 1s 22ms/step - loss: 0.0139\nEpoch 115/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0413\nEpoch 115: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0436\nEpoch 116/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0317\nEpoch 116: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0302\nEpoch 117/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0165\nEpoch 117: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0167\nEpoch 118/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0134\nEpoch 118: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0131\nEpoch 119/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0104\nEpoch 119: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0103\nEpoch 120/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0075\nEpoch 120: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0088\nEpoch 121/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0086\nEpoch 121: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0082\nEpoch 122/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0099\nEpoch 122: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0097\nEpoch 123/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0723\nEpoch 123: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0723\nEpoch 124/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0734\nEpoch 124: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0725\nEpoch 125/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0431\nEpoch 125: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0480\nEpoch 126/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0287\nEpoch 126: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0279\nEpoch 127/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0202\nEpoch 127: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0196\nEpoch 128/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0167\nEpoch 128: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0294\nEpoch 129/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0733\nEpoch 129: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0725\nEpoch 130/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0295\nEpoch 130: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0296\nEpoch 131/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0193\nEpoch 131: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0187\nEpoch 132/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0163\nEpoch 132: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0155\nEpoch 133/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0114\nEpoch 133: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0114\nEpoch 134/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0133\nEpoch 134: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0134\nEpoch 135/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0108\nEpoch 135: loss did not improve from 0.00551\n27/27 [==============================] - 1s 22ms/step - loss: 0.0108\nEpoch 136/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0086\nEpoch 136: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0090\nEpoch 137/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0092\nEpoch 137: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0094\nEpoch 138/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0077\nEpoch 138: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0078\nEpoch 139/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0072\nEpoch 139: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0069\nEpoch 140/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0075\nEpoch 140: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0089\nEpoch 141/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0084\nEpoch 141: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0079\nEpoch 142/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0061\nEpoch 142: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0078\nEpoch 143/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0075\nEpoch 143: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0075\nEpoch 144/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0080\nEpoch 144: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0079\nEpoch 145/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0069\nEpoch 145: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0066\nEpoch 146/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0065\nEpoch 146: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0065\nEpoch 147/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0064\nEpoch 147: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0064\nEpoch 148/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0075\nEpoch 148: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0072\nEpoch 149/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0046\nEpoch 149: loss did not improve from 0.00551\n27/27 [==============================] - 1s 21ms/step - loss: 0.0064\nEpoch 150/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0050\nEpoch 150: loss improved from 0.00551 to 0.00484, saving model to /content/drive/MyDrive/Проекты/ML/Pets/rnn_mario_level_builder/data/weights-improvement-150-0.0048.hdf5\n27/27 [==============================] - 1s 22ms/step - loss: 0.0048\nEpoch 151/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0065\nEpoch 151: loss did not improve from 0.00484\n27/27 [==============================] - 1s 21ms/step - loss: 0.0061\nEpoch 152/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0057\nEpoch 152: loss did not improve from 0.00484\n27/27 [==============================] - 1s 21ms/step - loss: 0.0057\nEpoch 153/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0052\nEpoch 153: loss did not improve from 0.00484\n27/27 [==============================] - 1s 21ms/step - loss: 0.0054\nEpoch 154/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0062\nEpoch 154: loss did not improve from 0.00484\n27/27 [==============================] - 1s 21ms/step - loss: 0.0059\nEpoch 155/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0077\nEpoch 155: loss did not improve from 0.00484\n27/27 [==============================] - 1s 21ms/step - loss: 0.0080\nEpoch 156/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0202\nEpoch 156: loss did not improve from 0.00484\n27/27 [==============================] - 1s 21ms/step - loss: 0.0211\nEpoch 157/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0249\nEpoch 157: loss did not improve from 0.00484\n27/27 [==============================] - 1s 21ms/step - loss: 0.0237\nEpoch 158/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0283\nEpoch 158: loss did not improve from 0.00484\n27/27 [==============================] - 1s 21ms/step - loss: 0.0299\nEpoch 159/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0144\nEpoch 159: loss did not improve from 0.00484\n27/27 [==============================] - 1s 21ms/step - loss: 0.0148\nEpoch 160/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0130\nEpoch 160: loss did not improve from 0.00484\n27/27 [==============================] - 1s 21ms/step - loss: 0.0125\nEpoch 161/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0072\nEpoch 161: loss did not improve from 0.00484\n27/27 [==============================] - 1s 21ms/step - loss: 0.0072\nEpoch 162/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0074\nEpoch 162: loss did not improve from 0.00484\n27/27 [==============================] - 1s 21ms/step - loss: 0.0089\nEpoch 163/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0071\nEpoch 163: loss did not improve from 0.00484\n27/27 [==============================] - 1s 21ms/step - loss: 0.0079\nEpoch 164/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0077\nEpoch 164: loss did not improve from 0.00484\n27/27 [==============================] - 1s 21ms/step - loss: 0.0075\nEpoch 165/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0071\nEpoch 165: loss did not improve from 0.00484\n27/27 [==============================] - 1s 21ms/step - loss: 0.0071\nEpoch 166/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0056\nEpoch 166: loss did not improve from 0.00484\n27/27 [==============================] - 1s 21ms/step - loss: 0.0054\nEpoch 167/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0053\nEpoch 167: loss did not improve from 0.00484\n27/27 [==============================] - 1s 21ms/step - loss: 0.0061\nEpoch 168/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0064\nEpoch 168: loss did not improve from 0.00484\n27/27 [==============================] - 1s 22ms/step - loss: 0.0064\nEpoch 169/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0057\nEpoch 169: loss did not improve from 0.00484\n27/27 [==============================] - 1s 21ms/step - loss: 0.0054\nEpoch 170/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0056\nEpoch 170: loss did not improve from 0.00484\n27/27 [==============================] - 1s 21ms/step - loss: 0.0053\nEpoch 171/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0048\nEpoch 171: loss did not improve from 0.00484\n27/27 [==============================] - 1s 21ms/step - loss: 0.0049\nEpoch 172/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0051\nEpoch 172: loss did not improve from 0.00484\n27/27 [==============================] - 1s 21ms/step - loss: 0.0058\nEpoch 173/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0044\nEpoch 173: loss did not improve from 0.00484\n27/27 [==============================] - 1s 20ms/step - loss: 0.0056\nEpoch 174/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0048\nEpoch 174: loss improved from 0.00484 to 0.00475, saving model to /content/drive/MyDrive/Проекты/ML/Pets/rnn_mario_level_builder/data/weights-improvement-174-0.0048.hdf5\n27/27 [==============================] - 1s 22ms/step - loss: 0.0048\nEpoch 175/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0055\nEpoch 175: loss did not improve from 0.00475\n27/27 [==============================] - 1s 21ms/step - loss: 0.0053\nEpoch 176/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0046\nEpoch 176: loss did not improve from 0.00475\n27/27 [==============================] - 1s 21ms/step - loss: 0.0053\nEpoch 177/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0118\nEpoch 177: loss did not improve from 0.00475\n27/27 [==============================] - 1s 21ms/step - loss: 0.0131\nEpoch 178/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0085\nEpoch 178: loss did not improve from 0.00475\n27/27 [==============================] - 1s 21ms/step - loss: 0.0081\nEpoch 179/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0145\nEpoch 179: loss did not improve from 0.00475\n27/27 [==============================] - 1s 21ms/step - loss: 0.0145\nEpoch 180/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0177\nEpoch 180: loss did not improve from 0.00475\n27/27 [==============================] - 1s 22ms/step - loss: 0.0181\nEpoch 181/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0124\nEpoch 181: loss did not improve from 0.00475\n27/27 [==============================] - 1s 21ms/step - loss: 0.0127\nEpoch 182/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0188\nEpoch 182: loss did not improve from 0.00475\n27/27 [==============================] - 1s 21ms/step - loss: 0.0179\nEpoch 183/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0167\nEpoch 183: loss did not improve from 0.00475\n27/27 [==============================] - 1s 21ms/step - loss: 0.0191\nEpoch 184/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0168\nEpoch 184: loss did not improve from 0.00475\n27/27 [==============================] - 1s 22ms/step - loss: 0.0179\nEpoch 185/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0188\nEpoch 185: loss did not improve from 0.00475\n27/27 [==============================] - 1s 21ms/step - loss: 0.0180\nEpoch 186/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0144\nEpoch 186: loss did not improve from 0.00475\n27/27 [==============================] - 1s 21ms/step - loss: 0.0136\nEpoch 187/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0084\nEpoch 187: loss did not improve from 0.00475\n27/27 [==============================] - 1s 21ms/step - loss: 0.0081\nEpoch 188/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0075\nEpoch 188: loss did not improve from 0.00475\n27/27 [==============================] - 1s 21ms/step - loss: 0.0072\nEpoch 189/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0065\nEpoch 189: loss did not improve from 0.00475\n27/27 [==============================] - 1s 21ms/step - loss: 0.0063\nEpoch 190/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0083\nEpoch 190: loss did not improve from 0.00475\n27/27 [==============================] - 1s 22ms/step - loss: 0.0083\nEpoch 191/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0070\nEpoch 191: loss did not improve from 0.00475\n27/27 [==============================] - 1s 21ms/step - loss: 0.0077\nEpoch 192/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0318\nEpoch 192: loss did not improve from 0.00475\n27/27 [==============================] - 1s 22ms/step - loss: 0.0367\nEpoch 193/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0636\nEpoch 193: loss did not improve from 0.00475\n27/27 [==============================] - 1s 21ms/step - loss: 0.0627\nEpoch 194/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0271\nEpoch 194: loss did not improve from 0.00475\n27/27 [==============================] - 1s 22ms/step - loss: 0.0278\nEpoch 195/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0160\nEpoch 195: loss did not improve from 0.00475\n27/27 [==============================] - 1s 21ms/step - loss: 0.0172\nEpoch 196/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0160\nEpoch 196: loss did not improve from 0.00475\n27/27 [==============================] - 1s 21ms/step - loss: 0.0153\nEpoch 197/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0099\nEpoch 197: loss did not improve from 0.00475\n27/27 [==============================] - 1s 21ms/step - loss: 0.0111\nEpoch 198/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0127\nEpoch 198: loss did not improve from 0.00475\n27/27 [==============================] - 1s 22ms/step - loss: 0.0128\nEpoch 199/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0113\nEpoch 199: loss did not improve from 0.00475\n27/27 [==============================] - 1s 22ms/step - loss: 0.0113\nEpoch 200/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0088\nEpoch 200: loss did not improve from 0.00475\n27/27 [==============================] - 1s 21ms/step - loss: 0.0083\nEpoch 201/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0075\nEpoch 201: loss did not improve from 0.00475\n27/27 [==============================] - 1s 22ms/step - loss: 0.0075\nEpoch 202/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0064\nEpoch 202: loss did not improve from 0.00475\n27/27 [==============================] - 1s 21ms/step - loss: 0.0064\nEpoch 203/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0063\nEpoch 203: loss did not improve from 0.00475\n27/27 [==============================] - 1s 21ms/step - loss: 0.0060\nEpoch 204/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0061\nEpoch 204: loss did not improve from 0.00475\n27/27 [==============================] - 1s 21ms/step - loss: 0.0058\nEpoch 205/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0049\nEpoch 205: loss did not improve from 0.00475\n27/27 [==============================] - 1s 21ms/step - loss: 0.0049\nEpoch 206/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0044\nEpoch 206: loss improved from 0.00475 to 0.00442, saving model to /content/drive/MyDrive/Проекты/ML/Pets/rnn_mario_level_builder/data/weights-improvement-206-0.0044.hdf5\n27/27 [==============================] - 1s 23ms/step - loss: 0.0044\nEpoch 207/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0058\nEpoch 207: loss did not improve from 0.00442\n27/27 [==============================] - 1s 21ms/step - loss: 0.0058\nEpoch 208/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0052\nEpoch 208: loss did not improve from 0.00442\n27/27 [==============================] - 1s 21ms/step - loss: 0.0049\nEpoch 209/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0058\nEpoch 209: loss did not improve from 0.00442\n27/27 [==============================] - 1s 22ms/step - loss: 0.0055\nEpoch 210/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0051\nEpoch 210: loss did not improve from 0.00442\n27/27 [==============================] - 1s 21ms/step - loss: 0.0049\nEpoch 211/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0070\nEpoch 211: loss did not improve from 0.00442\n27/27 [==============================] - 1s 21ms/step - loss: 0.0071\nEpoch 212/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0096\nEpoch 212: loss did not improve from 0.00442\n27/27 [==============================] - 1s 22ms/step - loss: 0.0096\nEpoch 213/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0166\nEpoch 213: loss did not improve from 0.00442\n27/27 [==============================] - 1s 22ms/step - loss: 0.0158\nEpoch 214/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0215\nEpoch 214: loss did not improve from 0.00442\n27/27 [==============================] - 1s 22ms/step - loss: 0.0219\nEpoch 215/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0160\nEpoch 215: loss did not improve from 0.00442\n27/27 [==============================] - 1s 21ms/step - loss: 0.0151\nEpoch 216/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0114\nEpoch 216: loss did not improve from 0.00442\n27/27 [==============================] - 1s 21ms/step - loss: 0.0122\nEpoch 217/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0227\nEpoch 217: loss did not improve from 0.00442\n27/27 [==============================] - 1s 21ms/step - loss: 0.0233\nEpoch 218/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0207\nEpoch 218: loss did not improve from 0.00442\n27/27 [==============================] - 1s 22ms/step - loss: 0.0216\nEpoch 219/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0174\nEpoch 219: loss did not improve from 0.00442\n27/27 [==============================] - 1s 21ms/step - loss: 0.0166\nEpoch 220/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0162\nEpoch 220: loss did not improve from 0.00442\n27/27 [==============================] - 1s 22ms/step - loss: 0.0162\nEpoch 221/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0121\nEpoch 221: loss did not improve from 0.00442\n27/27 [==============================] - 1s 22ms/step - loss: 0.0130\nEpoch 222/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0082\nEpoch 222: loss did not improve from 0.00442\n27/27 [==============================] - 1s 22ms/step - loss: 0.0080\nEpoch 223/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0072\nEpoch 223: loss did not improve from 0.00442\n27/27 [==============================] - 1s 22ms/step - loss: 0.0069\nEpoch 224/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0052\nEpoch 224: loss did not improve from 0.00442\n27/27 [==============================] - 1s 22ms/step - loss: 0.0053\nEpoch 225/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0045\nEpoch 225: loss did not improve from 0.00442\n27/27 [==============================] - 1s 22ms/step - loss: 0.0045\nEpoch 226/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0056\nEpoch 226: loss did not improve from 0.00442\n27/27 [==============================] - 1s 22ms/step - loss: 0.0053\nEpoch 227/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0051\nEpoch 227: loss did not improve from 0.00442\n27/27 [==============================] - 1s 22ms/step - loss: 0.0050\nEpoch 228/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0056\nEpoch 228: loss did not improve from 0.00442\n27/27 [==============================] - 1s 21ms/step - loss: 0.0053\nEpoch 229/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0044\nEpoch 229: loss improved from 0.00442 to 0.00417, saving model to /content/drive/MyDrive/Проекты/ML/Pets/rnn_mario_level_builder/data/weights-improvement-229-0.0042.hdf5\n27/27 [==============================] - 1s 23ms/step - loss: 0.0042\nEpoch 230/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0046\nEpoch 230: loss did not improve from 0.00417\n27/27 [==============================] - 1s 22ms/step - loss: 0.0044\nEpoch 231/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0041\nEpoch 231: loss improved from 0.00417 to 0.00410, saving model to /content/drive/MyDrive/Проекты/ML/Pets/rnn_mario_level_builder/data/weights-improvement-231-0.0041.hdf5\n27/27 [==============================] - 1s 23ms/step - loss: 0.0041\nEpoch 232/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0054\nEpoch 232: loss did not improve from 0.00410\n27/27 [==============================] - 1s 22ms/step - loss: 0.0051\nEpoch 233/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0047\nEpoch 233: loss did not improve from 0.00410\n27/27 [==============================] - 1s 21ms/step - loss: 0.0048\nEpoch 234/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0049\nEpoch 234: loss did not improve from 0.00410\n27/27 [==============================] - 1s 22ms/step - loss: 0.0047\nEpoch 235/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0052\nEpoch 235: loss did not improve from 0.00410\n27/27 [==============================] - 1s 22ms/step - loss: 0.0052\nEpoch 236/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0047\nEpoch 236: loss did not improve from 0.00410\n27/27 [==============================] - 1s 22ms/step - loss: 0.0044\nEpoch 237/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0044\nEpoch 237: loss did not improve from 0.00410\n27/27 [==============================] - 1s 22ms/step - loss: 0.0042\nEpoch 238/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0054\nEpoch 238: loss did not improve from 0.00410\n27/27 [==============================] - 1s 22ms/step - loss: 0.0054\nEpoch 239/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0039\nEpoch 239: loss did not improve from 0.00410\n27/27 [==============================] - 1s 21ms/step - loss: 0.0041\nEpoch 240/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0042\nEpoch 240: loss improved from 0.00410 to 0.00402, saving model to /content/drive/MyDrive/Проекты/ML/Pets/rnn_mario_level_builder/data/weights-improvement-240-0.0040.hdf5\n27/27 [==============================] - 1s 23ms/step - loss: 0.0040\nEpoch 241/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0043\nEpoch 241: loss did not improve from 0.00402\n27/27 [==============================] - 1s 21ms/step - loss: 0.0041\nEpoch 242/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0046\nEpoch 242: loss did not improve from 0.00402\n27/27 [==============================] - 1s 22ms/step - loss: 0.0044\nEpoch 243/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0037\nEpoch 243: loss did not improve from 0.00402\n27/27 [==============================] - 1s 21ms/step - loss: 0.0046\nEpoch 244/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0043\nEpoch 244: loss did not improve from 0.00402\n27/27 [==============================] - 1s 21ms/step - loss: 0.0041\nEpoch 245/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0050\nEpoch 245: loss did not improve from 0.00402\n27/27 [==============================] - 1s 21ms/step - loss: 0.0047\nEpoch 246/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0042\nEpoch 246: loss improved from 0.00402 to 0.00394, saving model to /content/drive/MyDrive/Проекты/ML/Pets/rnn_mario_level_builder/data/weights-improvement-246-0.0039.hdf5\n27/27 [==============================] - 1s 23ms/step - loss: 0.0039\nEpoch 247/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0047\nEpoch 247: loss did not improve from 0.00394\n27/27 [==============================] - 1s 21ms/step - loss: 0.0044\nEpoch 248/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0114\nEpoch 248: loss did not improve from 0.00394\n27/27 [==============================] - 1s 21ms/step - loss: 0.0117\nEpoch 249/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0309\nEpoch 249: loss did not improve from 0.00394\n27/27 [==============================] - 1s 22ms/step - loss: 0.0331\nEpoch 250/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0324\nEpoch 250: loss did not improve from 0.00394\n27/27 [==============================] - 1s 21ms/step - loss: 0.0333\nEpoch 251/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0501\nEpoch 251: loss did not improve from 0.00394\n27/27 [==============================] - 1s 22ms/step - loss: 0.0585\nEpoch 252/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0696\nEpoch 252: loss did not improve from 0.00394\n27/27 [==============================] - 1s 22ms/step - loss: 0.0696\nEpoch 253/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0301\nEpoch 253: loss did not improve from 0.00394\n27/27 [==============================] - 1s 21ms/step - loss: 0.0292\nEpoch 254/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0232\nEpoch 254: loss did not improve from 0.00394\n27/27 [==============================] - 1s 22ms/step - loss: 0.0258\nEpoch 255/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0200\nEpoch 255: loss did not improve from 0.00394\n27/27 [==============================] - 1s 23ms/step - loss: 0.0195\nEpoch 256/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0156\nEpoch 256: loss did not improve from 0.00394\n27/27 [==============================] - 1s 22ms/step - loss: 0.0156\nEpoch 257/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0087\nEpoch 257: loss did not improve from 0.00394\n27/27 [==============================] - 1s 22ms/step - loss: 0.0094\nEpoch 258/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0079\nEpoch 258: loss did not improve from 0.00394\n27/27 [==============================] - 1s 22ms/step - loss: 0.0079\nEpoch 259/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0081\nEpoch 259: loss did not improve from 0.00394\n27/27 [==============================] - 1s 22ms/step - loss: 0.0079\nEpoch 260/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0087\nEpoch 260: loss did not improve from 0.00394\n27/27 [==============================] - 1s 21ms/step - loss: 0.0088\nEpoch 261/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0084\nEpoch 261: loss did not improve from 0.00394\n27/27 [==============================] - 1s 22ms/step - loss: 0.0080\nEpoch 262/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0109\nEpoch 262: loss did not improve from 0.00394\n27/27 [==============================] - 1s 22ms/step - loss: 0.0112\nEpoch 263/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0106\nEpoch 263: loss did not improve from 0.00394\n27/27 [==============================] - 1s 22ms/step - loss: 0.0102\nEpoch 264/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0111\nEpoch 264: loss did not improve from 0.00394\n27/27 [==============================] - 1s 22ms/step - loss: 0.0111\nEpoch 265/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0132\nEpoch 265: loss did not improve from 0.00394\n27/27 [==============================] - 1s 22ms/step - loss: 0.0134\nEpoch 266/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0168\nEpoch 266: loss did not improve from 0.00394\n27/27 [==============================] - 1s 22ms/step - loss: 0.0168\nEpoch 267/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0158\nEpoch 267: loss did not improve from 0.00394\n27/27 [==============================] - 1s 23ms/step - loss: 0.0158\nEpoch 268/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0081\nEpoch 268: loss did not improve from 0.00394\n27/27 [==============================] - 1s 22ms/step - loss: 0.0079\nEpoch 269/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0067\nEpoch 269: loss did not improve from 0.00394\n27/27 [==============================] - 1s 22ms/step - loss: 0.0067\nEpoch 270/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0055\nEpoch 270: loss did not improve from 0.00394\n27/27 [==============================] - 1s 22ms/step - loss: 0.0055\nEpoch 271/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0055\nEpoch 271: loss did not improve from 0.00394\n27/27 [==============================] - 1s 22ms/step - loss: 0.0053\nEpoch 272/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0045\nEpoch 272: loss did not improve from 0.00394\n27/27 [==============================] - 1s 21ms/step - loss: 0.0043\nEpoch 273/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0050\nEpoch 273: loss did not improve from 0.00394\n27/27 [==============================] - 1s 21ms/step - loss: 0.0048\nEpoch 274/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0051\nEpoch 274: loss did not improve from 0.00394\n27/27 [==============================] - 1s 22ms/step - loss: 0.0051\nEpoch 275/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0050\nEpoch 275: loss did not improve from 0.00394\n27/27 [==============================] - 1s 21ms/step - loss: 0.0047\nEpoch 276/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0053\nEpoch 276: loss did not improve from 0.00394\n27/27 [==============================] - 1s 22ms/step - loss: 0.0051\nEpoch 277/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0045\nEpoch 277: loss did not improve from 0.00394\n27/27 [==============================] - 1s 21ms/step - loss: 0.0043\nEpoch 278/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0051\nEpoch 278: loss did not improve from 0.00394\n27/27 [==============================] - 1s 22ms/step - loss: 0.0048\nEpoch 279/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0048\nEpoch 279: loss did not improve from 0.00394\n27/27 [==============================] - 1s 21ms/step - loss: 0.0046\nEpoch 280/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0048\nEpoch 280: loss did not improve from 0.00394\n27/27 [==============================] - 1s 22ms/step - loss: 0.0048\nEpoch 281/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0059\nEpoch 281: loss did not improve from 0.00394\n27/27 [==============================] - 1s 21ms/step - loss: 0.0056\nEpoch 282/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0041\nEpoch 282: loss improved from 0.00394 to 0.00385, saving model to /content/drive/MyDrive/Проекты/ML/Pets/rnn_mario_level_builder/data/weights-improvement-282-0.0039.hdf5\n27/27 [==============================] - 1s 23ms/step - loss: 0.0039\nEpoch 283/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0035\nEpoch 283: loss did not improve from 0.00385\n27/27 [==============================] - 1s 21ms/step - loss: 0.0042\nEpoch 284/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0046\nEpoch 284: loss did not improve from 0.00385\n27/27 [==============================] - 1s 21ms/step - loss: 0.0044\nEpoch 285/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0055\nEpoch 285: loss did not improve from 0.00385\n27/27 [==============================] - 1s 22ms/step - loss: 0.0052\nEpoch 286/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0064\nEpoch 286: loss did not improve from 0.00385\n27/27 [==============================] - 1s 21ms/step - loss: 0.0060\nEpoch 287/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0041\nEpoch 287: loss did not improve from 0.00385\n27/27 [==============================] - 1s 22ms/step - loss: 0.0039\nEpoch 288/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0051\nEpoch 288: loss did not improve from 0.00385\n27/27 [==============================] - 1s 22ms/step - loss: 0.0048\nEpoch 289/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0051\nEpoch 289: loss did not improve from 0.00385\n27/27 [==============================] - 1s 22ms/step - loss: 0.0051\nEpoch 290/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0067\nEpoch 290: loss did not improve from 0.00385\n27/27 [==============================] - 1s 23ms/step - loss: 0.0065\nEpoch 291/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0051\nEpoch 291: loss did not improve from 0.00385\n27/27 [==============================] - 1s 22ms/step - loss: 0.0049\nEpoch 292/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0048\nEpoch 292: loss did not improve from 0.00385\n27/27 [==============================] - 1s 22ms/step - loss: 0.0047\nEpoch 293/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0072\nEpoch 293: loss did not improve from 0.00385\n27/27 [==============================] - 1s 22ms/step - loss: 0.0072\nEpoch 294/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0084\nEpoch 294: loss did not improve from 0.00385\n27/27 [==============================] - 1s 22ms/step - loss: 0.0083\nEpoch 295/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0112\nEpoch 295: loss did not improve from 0.00385\n27/27 [==============================] - 1s 22ms/step - loss: 0.0112\nEpoch 296/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0097\nEpoch 296: loss did not improve from 0.00385\n27/27 [==============================] - 1s 22ms/step - loss: 0.0104\nEpoch 297/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0290\nEpoch 297: loss did not improve from 0.00385\n27/27 [==============================] - 1s 22ms/step - loss: 0.0301\nEpoch 298/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0331\nEpoch 298: loss did not improve from 0.00385\n27/27 [==============================] - 1s 22ms/step - loss: 0.0321\nEpoch 299/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0269\nEpoch 299: loss did not improve from 0.00385\n27/27 [==============================] - 1s 22ms/step - loss: 0.0269\nEpoch 300/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0217\nEpoch 300: loss did not improve from 0.00385\n27/27 [==============================] - 1s 22ms/step - loss: 0.0220\nEpoch 301/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0180\nEpoch 301: loss did not improve from 0.00385\n27/27 [==============================] - 1s 21ms/step - loss: 0.0173\nEpoch 302/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0145\nEpoch 302: loss did not improve from 0.00385\n27/27 [==============================] - 1s 22ms/step - loss: 0.0151\nEpoch 303/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0119\nEpoch 303: loss did not improve from 0.00385\n27/27 [==============================] - 1s 22ms/step - loss: 0.0118\nEpoch 304/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0083\nEpoch 304: loss did not improve from 0.00385\n27/27 [==============================] - 1s 22ms/step - loss: 0.0079\nEpoch 305/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0110\nEpoch 305: loss did not improve from 0.00385\n27/27 [==============================] - 1s 22ms/step - loss: 0.0108\nEpoch 306/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0201\nEpoch 306: loss did not improve from 0.00385\n27/27 [==============================] - 1s 22ms/step - loss: 0.0201\nEpoch 307/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0133\nEpoch 307: loss did not improve from 0.00385\n27/27 [==============================] - 1s 22ms/step - loss: 0.0143\nEpoch 308/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0123\nEpoch 308: loss did not improve from 0.00385\n27/27 [==============================] - 1s 22ms/step - loss: 0.0132\nEpoch 309/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0126\nEpoch 309: loss did not improve from 0.00385\n27/27 [==============================] - 1s 22ms/step - loss: 0.0119\nEpoch 310/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0084\nEpoch 310: loss did not improve from 0.00385\n27/27 [==============================] - 1s 22ms/step - loss: 0.0097\nEpoch 311/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0184\nEpoch 311: loss did not improve from 0.00385\n27/27 [==============================] - 1s 22ms/step - loss: 0.0177\nEpoch 312/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0084\nEpoch 312: loss did not improve from 0.00385\n27/27 [==============================] - 1s 22ms/step - loss: 0.0096\nEpoch 313/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0107\nEpoch 313: loss did not improve from 0.00385\n27/27 [==============================] - 1s 22ms/step - loss: 0.0109\nEpoch 314/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0074\nEpoch 314: loss did not improve from 0.00385\n27/27 [==============================] - 1s 22ms/step - loss: 0.0072\nEpoch 315/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0061\nEpoch 315: loss did not improve from 0.00385\n27/27 [==============================] - 1s 23ms/step - loss: 0.0061\nEpoch 316/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0050\nEpoch 316: loss did not improve from 0.00385\n27/27 [==============================] - 1s 22ms/step - loss: 0.0050\nEpoch 317/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0043\nEpoch 317: loss did not improve from 0.00385\n27/27 [==============================] - 1s 22ms/step - loss: 0.0043\nEpoch 318/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0044\nEpoch 318: loss did not improve from 0.00385\n27/27 [==============================] - 1s 22ms/step - loss: 0.0042\nEpoch 319/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0039\nEpoch 319: loss did not improve from 0.00385\n27/27 [==============================] - 1s 22ms/step - loss: 0.0039\nEpoch 320/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0036\nEpoch 320: loss improved from 0.00385 to 0.00355, saving model to /content/drive/MyDrive/Проекты/ML/Pets/rnn_mario_level_builder/data/weights-improvement-320-0.0036.hdf5\n27/27 [==============================] - 1s 24ms/step - loss: 0.0036\nEpoch 321/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0043\nEpoch 321: loss did not improve from 0.00355\n27/27 [==============================] - 1s 21ms/step - loss: 0.0041\nEpoch 322/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0041\nEpoch 322: loss did not improve from 0.00355\n27/27 [==============================] - 1s 22ms/step - loss: 0.0041\nEpoch 323/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0041\nEpoch 323: loss did not improve from 0.00355\n27/27 [==============================] - 1s 22ms/step - loss: 0.0039\nEpoch 324/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0030\nEpoch 324: loss did not improve from 0.00355\n27/27 [==============================] - 1s 22ms/step - loss: 0.0044\nEpoch 325/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0044\nEpoch 325: loss did not improve from 0.00355\n27/27 [==============================] - 1s 23ms/step - loss: 0.0043\nEpoch 326/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0047\nEpoch 326: loss did not improve from 0.00355\n27/27 [==============================] - 1s 22ms/step - loss: 0.0047\nEpoch 327/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0049\nEpoch 327: loss did not improve from 0.00355\n27/27 [==============================] - 1s 21ms/step - loss: 0.0046\nEpoch 328/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0030\nEpoch 328: loss did not improve from 0.00355\n27/27 [==============================] - 1s 21ms/step - loss: 0.0038\nEpoch 329/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0041\nEpoch 329: loss did not improve from 0.00355\n27/27 [==============================] - 1s 22ms/step - loss: 0.0039\nEpoch 330/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0044\nEpoch 330: loss did not improve from 0.00355\n27/27 [==============================] - 1s 22ms/step - loss: 0.0044\nEpoch 331/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0042\nEpoch 331: loss did not improve from 0.00355\n27/27 [==============================] - 1s 22ms/step - loss: 0.0039\nEpoch 332/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0040\nEpoch 332: loss did not improve from 0.00355\n27/27 [==============================] - 1s 22ms/step - loss: 0.0054\nEpoch 333/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0090\nEpoch 333: loss did not improve from 0.00355\n27/27 [==============================] - 1s 22ms/step - loss: 0.0090\nEpoch 334/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0301\nEpoch 334: loss did not improve from 0.00355\n27/27 [==============================] - 1s 22ms/step - loss: 0.0301\nEpoch 335/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0329\nEpoch 335: loss did not improve from 0.00355\n27/27 [==============================] - 1s 22ms/step - loss: 0.0330\nEpoch 336/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0341\nEpoch 336: loss did not improve from 0.00355\n27/27 [==============================] - 1s 22ms/step - loss: 0.0322\nEpoch 337/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0272\nEpoch 337: loss did not improve from 0.00355\n27/27 [==============================] - 1s 22ms/step - loss: 0.0271\nEpoch 338/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0135\nEpoch 338: loss did not improve from 0.00355\n27/27 [==============================] - 1s 22ms/step - loss: 0.0135\nEpoch 339/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0117\nEpoch 339: loss did not improve from 0.00355\n27/27 [==============================] - 1s 22ms/step - loss: 0.0117\nEpoch 340/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0085\nEpoch 340: loss did not improve from 0.00355\n27/27 [==============================] - 1s 22ms/step - loss: 0.0085\nEpoch 341/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0078\nEpoch 341: loss did not improve from 0.00355\n27/27 [==============================] - 1s 22ms/step - loss: 0.0078\nEpoch 342/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0069\nEpoch 342: loss did not improve from 0.00355\n27/27 [==============================] - 1s 22ms/step - loss: 0.0065\nEpoch 343/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0075\nEpoch 343: loss did not improve from 0.00355\n27/27 [==============================] - 1s 22ms/step - loss: 0.0071\nEpoch 344/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0066\nEpoch 344: loss did not improve from 0.00355\n27/27 [==============================] - 1s 22ms/step - loss: 0.0080\nEpoch 345/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0102\nEpoch 345: loss did not improve from 0.00355\n27/27 [==============================] - 1s 21ms/step - loss: 0.0096\nEpoch 346/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0044\nEpoch 346: loss did not improve from 0.00355\n27/27 [==============================] - 1s 22ms/step - loss: 0.0059\nEpoch 347/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0039\nEpoch 347: loss did not improve from 0.00355\n27/27 [==============================] - 1s 22ms/step - loss: 0.0039\nEpoch 348/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0042\nEpoch 348: loss did not improve from 0.00355\n27/27 [==============================] - 1s 21ms/step - loss: 0.0041\nEpoch 349/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0041\nEpoch 349: loss did not improve from 0.00355\n27/27 [==============================] - 1s 23ms/step - loss: 0.0041\nEpoch 350/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0050\nEpoch 350: loss did not improve from 0.00355\n27/27 [==============================] - 1s 22ms/step - loss: 0.0050\nEpoch 351/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0039\nEpoch 351: loss did not improve from 0.00355\n27/27 [==============================] - 1s 22ms/step - loss: 0.0038\nEpoch 352/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0037\nEpoch 352: loss did not improve from 0.00355\n27/27 [==============================] - 1s 22ms/step - loss: 0.0037\nEpoch 353/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0042\nEpoch 353: loss did not improve from 0.00355\n27/27 [==============================] - 1s 22ms/step - loss: 0.0042\nEpoch 354/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0044\nEpoch 354: loss did not improve from 0.00355\n27/27 [==============================] - 1s 22ms/step - loss: 0.0041\nEpoch 355/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0040\nEpoch 355: loss did not improve from 0.00355\n27/27 [==============================] - 1s 22ms/step - loss: 0.0038\nEpoch 356/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0037\nEpoch 356: loss did not improve from 0.00355\n27/27 [==============================] - 1s 22ms/step - loss: 0.0036\nEpoch 357/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0038\nEpoch 357: loss did not improve from 0.00355\n27/27 [==============================] - 1s 21ms/step - loss: 0.0036\nEpoch 358/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0038\nEpoch 358: loss did not improve from 0.00355\n27/27 [==============================] - 1s 22ms/step - loss: 0.0037\nEpoch 359/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0040\nEpoch 359: loss did not improve from 0.00355\n27/27 [==============================] - 1s 22ms/step - loss: 0.0040\nEpoch 360/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0036\nEpoch 360: loss improved from 0.00355 to 0.00348, saving model to /content/drive/MyDrive/Проекты/ML/Pets/rnn_mario_level_builder/data/weights-improvement-360-0.0035.hdf5\n27/27 [==============================] - 1s 24ms/step - loss: 0.0035\nEpoch 361/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0036\nEpoch 361: loss did not improve from 0.00348\n27/27 [==============================] - 1s 22ms/step - loss: 0.0036\nEpoch 362/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0035\nEpoch 362: loss improved from 0.00348 to 0.00334, saving model to /content/drive/MyDrive/Проекты/ML/Pets/rnn_mario_level_builder/data/weights-improvement-362-0.0033.hdf5\n27/27 [==============================] - 1s 23ms/step - loss: 0.0033\nEpoch 363/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0045\nEpoch 363: loss did not improve from 0.00334\n27/27 [==============================] - 1s 22ms/step - loss: 0.0042\nEpoch 364/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0043\nEpoch 364: loss did not improve from 0.00334\n27/27 [==============================] - 1s 22ms/step - loss: 0.0042\nEpoch 365/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0040\nEpoch 365: loss did not improve from 0.00334\n27/27 [==============================] - 1s 22ms/step - loss: 0.0040\nEpoch 366/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0037\nEpoch 366: loss did not improve from 0.00334\n27/27 [==============================] - 1s 22ms/step - loss: 0.0037\nEpoch 367/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0033\nEpoch 367: loss improved from 0.00334 to 0.00313, saving model to /content/drive/MyDrive/Проекты/ML/Pets/rnn_mario_level_builder/data/weights-improvement-367-0.0031.hdf5\n27/27 [==============================] - 1s 24ms/step - loss: 0.0031\nEpoch 368/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0033\nEpoch 368: loss did not improve from 0.00313\n27/27 [==============================] - 1s 22ms/step - loss: 0.0033\nEpoch 369/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0037\nEpoch 369: loss did not improve from 0.00313\n27/27 [==============================] - 1s 22ms/step - loss: 0.0037\nEpoch 370/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0036\nEpoch 370: loss did not improve from 0.00313\n27/27 [==============================] - 1s 22ms/step - loss: 0.0036\nEpoch 371/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0030\nEpoch 371: loss improved from 0.00313 to 0.00285, saving model to /content/drive/MyDrive/Проекты/ML/Pets/rnn_mario_level_builder/data/weights-improvement-371-0.0029.hdf5\n27/27 [==============================] - 1s 24ms/step - loss: 0.0029\nEpoch 372/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0032\nEpoch 372: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0032\nEpoch 373/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0038\nEpoch 373: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0036\nEpoch 374/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0037\nEpoch 374: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0036\nEpoch 375/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0039\nEpoch 375: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0039\nEpoch 376/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0030\nEpoch 376: loss did not improve from 0.00285\n27/27 [==============================] - 1s 23ms/step - loss: 0.0029\nEpoch 377/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0034\nEpoch 377: loss did not improve from 0.00285\n27/27 [==============================] - 1s 23ms/step - loss: 0.0033\nEpoch 378/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0033\nEpoch 378: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0031\nEpoch 379/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0152\nEpoch 379: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0152\nEpoch 380/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0475\nEpoch 380: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0465\nEpoch 381/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0303\nEpoch 381: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0292\nEpoch 382/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0280\nEpoch 382: loss did not improve from 0.00285\n27/27 [==============================] - 1s 23ms/step - loss: 0.0280\nEpoch 383/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0256\nEpoch 383: loss did not improve from 0.00285\n27/27 [==============================] - 1s 23ms/step - loss: 0.0277\nEpoch 384/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0220\nEpoch 384: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0226\nEpoch 385/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0188\nEpoch 385: loss did not improve from 0.00285\n27/27 [==============================] - 1s 23ms/step - loss: 0.0188\nEpoch 386/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0373\nEpoch 386: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0373\nEpoch 387/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0353\nEpoch 387: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0345\nEpoch 388/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0262\nEpoch 388: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0257\nEpoch 389/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0209\nEpoch 389: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0209\nEpoch 390/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0157\nEpoch 390: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0153\nEpoch 391/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0092\nEpoch 391: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0087\nEpoch 392/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0102\nEpoch 392: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0108\nEpoch 393/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0173\nEpoch 393: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0182\nEpoch 394/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0099\nEpoch 394: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0099\nEpoch 395/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0102\nEpoch 395: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0102\nEpoch 396/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0107\nEpoch 396: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0105\nEpoch 397/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0086\nEpoch 397: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0083\nEpoch 398/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0068\nEpoch 398: loss did not improve from 0.00285\n27/27 [==============================] - 1s 23ms/step - loss: 0.0066\nEpoch 399/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0077\nEpoch 399: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0077\nEpoch 400/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0040\nEpoch 400: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0049\nEpoch 401/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0051\nEpoch 401: loss did not improve from 0.00285\n27/27 [==============================] - 1s 23ms/step - loss: 0.0050\nEpoch 402/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0118\nEpoch 402: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0118\nEpoch 403/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0094\nEpoch 403: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0094\nEpoch 404/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0076\nEpoch 404: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0073\nEpoch 405/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0053\nEpoch 405: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0050\nEpoch 406/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0042\nEpoch 406: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0043\nEpoch 407/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0047\nEpoch 407: loss did not improve from 0.00285\n27/27 [==============================] - 1s 21ms/step - loss: 0.0045\nEpoch 408/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0057\nEpoch 408: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0058\nEpoch 409/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0034\nEpoch 409: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0049\nEpoch 410/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0038\nEpoch 410: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0037\nEpoch 411/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0050\nEpoch 411: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0047\nEpoch 412/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0053\nEpoch 412: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0064\nEpoch 413/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0119\nEpoch 413: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0116\nEpoch 414/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0209\nEpoch 414: loss did not improve from 0.00285\n27/27 [==============================] - 1s 23ms/step - loss: 0.0209\nEpoch 415/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0203\nEpoch 415: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0199\nEpoch 416/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0115\nEpoch 416: loss did not improve from 0.00285\n27/27 [==============================] - 1s 23ms/step - loss: 0.0112\nEpoch 417/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0148\nEpoch 417: loss did not improve from 0.00285\n27/27 [==============================] - 1s 23ms/step - loss: 0.0148\nEpoch 418/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0172\nEpoch 418: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0181\nEpoch 419/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0101\nEpoch 419: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0118\nEpoch 420/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0147\nEpoch 420: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0140\nEpoch 421/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0144\nEpoch 421: loss did not improve from 0.00285\n27/27 [==============================] - 1s 23ms/step - loss: 0.0145\nEpoch 422/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0221\nEpoch 422: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0212\nEpoch 423/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0167\nEpoch 423: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0159\nEpoch 424/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0098\nEpoch 424: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0096\nEpoch 425/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0109\nEpoch 425: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0109\nEpoch 426/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0063\nEpoch 426: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0068\nEpoch 427/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0056\nEpoch 427: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0054\nEpoch 428/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0052\nEpoch 428: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0049\nEpoch 429/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0043\nEpoch 429: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0043\nEpoch 430/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0045\nEpoch 430: loss did not improve from 0.00285\n27/27 [==============================] - 1s 23ms/step - loss: 0.0045\nEpoch 431/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0039\nEpoch 431: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0037\nEpoch 432/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0036\nEpoch 432: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0036\nEpoch 433/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0036\nEpoch 433: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0036\nEpoch 434/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0036\nEpoch 434: loss did not improve from 0.00285\n27/27 [==============================] - 1s 21ms/step - loss: 0.0037\nEpoch 435/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0039\nEpoch 435: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0038\nEpoch 436/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0039\nEpoch 436: loss did not improve from 0.00285\n27/27 [==============================] - 1s 21ms/step - loss: 0.0037\nEpoch 437/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0040\nEpoch 437: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0038\nEpoch 438/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0039\nEpoch 438: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0037\nEpoch 439/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0040\nEpoch 439: loss did not improve from 0.00285\n27/27 [==============================] - 1s 21ms/step - loss: 0.0037\nEpoch 440/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0037\nEpoch 440: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0035\nEpoch 441/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0039\nEpoch 441: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0039\nEpoch 442/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0031\nEpoch 442: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0030\nEpoch 443/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0034\nEpoch 443: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0034\nEpoch 444/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0033\nEpoch 444: loss did not improve from 0.00285\n27/27 [==============================] - 1s 23ms/step - loss: 0.0033\nEpoch 445/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0031\nEpoch 445: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0030\nEpoch 446/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0035\nEpoch 446: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0035\nEpoch 447/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0035\nEpoch 447: loss did not improve from 0.00285\n27/27 [==============================] - 1s 23ms/step - loss: 0.0034\nEpoch 448/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0033\nEpoch 448: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0033\nEpoch 449/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0037\nEpoch 449: loss did not improve from 0.00285\n27/27 [==============================] - 1s 23ms/step - loss: 0.0037\nEpoch 450/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0035\nEpoch 450: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0033\nEpoch 451/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0034\nEpoch 451: loss did not improve from 0.00285\n27/27 [==============================] - 1s 23ms/step - loss: 0.0034\nEpoch 452/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0033\nEpoch 452: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0033\nEpoch 453/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0033\nEpoch 453: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0033\nEpoch 454/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0045\nEpoch 454: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0045\nEpoch 455/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0038\nEpoch 455: loss did not improve from 0.00285\n27/27 [==============================] - 1s 22ms/step - loss: 0.0038\nEpoch 456/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0024\nEpoch 456: loss improved from 0.00285 to 0.00240, saving model to /content/drive/MyDrive/Проекты/ML/Pets/rnn_mario_level_builder/data/weights-improvement-456-0.0024.hdf5\n27/27 [==============================] - 1s 24ms/step - loss: 0.0024\nEpoch 457/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0037\nEpoch 457: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0035\nEpoch 458/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0034\nEpoch 458: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0033\nEpoch 459/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0036\nEpoch 459: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0035\nEpoch 460/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0029\nEpoch 460: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0029\nEpoch 461/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0031\nEpoch 461: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0031\nEpoch 462/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0030\nEpoch 462: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0030\nEpoch 463/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0038\nEpoch 463: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0036\nEpoch 464/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0038\nEpoch 464: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0038\nEpoch 465/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0032\nEpoch 465: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0032\nEpoch 466/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0030\nEpoch 466: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0036\nEpoch 467/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0031\nEpoch 467: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0030\nEpoch 468/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0025\nEpoch 468: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0034\nEpoch 469/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0033\nEpoch 469: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0031\nEpoch 470/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0032\nEpoch 470: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0032\nEpoch 471/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0036\nEpoch 471: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0034\nEpoch 472/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0033\nEpoch 472: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0033\nEpoch 473/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0036\nEpoch 473: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0035\nEpoch 474/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0038\nEpoch 474: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0036\nEpoch 475/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0031\nEpoch 475: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0030\nEpoch 476/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0034\nEpoch 476: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0034\nEpoch 477/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0027\nEpoch 477: loss did not improve from 0.00240\n27/27 [==============================] - 1s 24ms/step - loss: 0.0029\nEpoch 478/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0033\nEpoch 478: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0033\nEpoch 479/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0052\nEpoch 479: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0050\nEpoch 480/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0246\nEpoch 480: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0241\nEpoch 481/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0511\nEpoch 481: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0503\nEpoch 482/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0438\nEpoch 482: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0438\nEpoch 483/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0711\nEpoch 483: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0721\nEpoch 484/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0330\nEpoch 484: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0317\nEpoch 485/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0293\nEpoch 485: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0293\nEpoch 486/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0158\nEpoch 486: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0174\nEpoch 487/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0253\nEpoch 487: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0253\nEpoch 488/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0226\nEpoch 488: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0234\nEpoch 489/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0300\nEpoch 489: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0300\nEpoch 490/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0307\nEpoch 490: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0333\nEpoch 491/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0390\nEpoch 491: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0390\nEpoch 492/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0266\nEpoch 492: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0259\nEpoch 493/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0159\nEpoch 493: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0156\nEpoch 494/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0162\nEpoch 494: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0162\nEpoch 495/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0134\nEpoch 495: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0142\nEpoch 496/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0130\nEpoch 496: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0124\nEpoch 497/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0103\nEpoch 497: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0097\nEpoch 498/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0106\nEpoch 498: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0105\nEpoch 499/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0085\nEpoch 499: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0085\nEpoch 500/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0079\nEpoch 500: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0076\nEpoch 501/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0055\nEpoch 501: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0054\nEpoch 502/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0067\nEpoch 502: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0067\nEpoch 503/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0057\nEpoch 503: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0057\nEpoch 504/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0062\nEpoch 504: loss did not improve from 0.00240\n27/27 [==============================] - 1s 21ms/step - loss: 0.0062\nEpoch 505/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0042\nEpoch 505: loss did not improve from 0.00240\n27/27 [==============================] - 1s 21ms/step - loss: 0.0044\nEpoch 506/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0048\nEpoch 506: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0051\nEpoch 507/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0062\nEpoch 507: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0073\nEpoch 508/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0068\nEpoch 508: loss did not improve from 0.00240\n27/27 [==============================] - 1s 24ms/step - loss: 0.0067\nEpoch 509/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0054\nEpoch 509: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0054\nEpoch 510/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0049\nEpoch 510: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0049\nEpoch 511/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0044\nEpoch 511: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0043\nEpoch 512/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0036\nEpoch 512: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0036\nEpoch 513/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0042\nEpoch 513: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0041\nEpoch 514/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0038\nEpoch 514: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0038\nEpoch 515/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0035\nEpoch 515: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0035\nEpoch 516/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0037\nEpoch 516: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0036\nEpoch 517/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0043\nEpoch 517: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0043\nEpoch 518/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0039\nEpoch 518: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0037\nEpoch 519/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0035\nEpoch 519: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0033\nEpoch 520/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0024\nEpoch 520: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0032\nEpoch 521/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0033\nEpoch 521: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0033\nEpoch 522/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0036\nEpoch 522: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0036\nEpoch 523/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0032\nEpoch 523: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0032\nEpoch 524/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0042\nEpoch 524: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0041\nEpoch 525/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0042\nEpoch 525: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0042\nEpoch 526/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0036\nEpoch 526: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0036\nEpoch 527/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0031\nEpoch 527: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0030\nEpoch 528/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0034\nEpoch 528: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0033\nEpoch 529/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0038\nEpoch 529: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0038\nEpoch 530/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0067\nEpoch 530: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0063\nEpoch 531/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0558\nEpoch 531: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0560\nEpoch 532/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0483\nEpoch 532: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0478\nEpoch 533/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0336\nEpoch 533: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0325\nEpoch 534/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0149\nEpoch 534: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0159\nEpoch 535/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0135\nEpoch 535: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0152\nEpoch 536/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0146\nEpoch 536: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0144\nEpoch 537/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0104\nEpoch 537: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0124\nEpoch 538/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0091\nEpoch 538: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0091\nEpoch 539/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0081\nEpoch 539: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0079\nEpoch 540/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0059\nEpoch 540: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0069\nEpoch 541/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0077\nEpoch 541: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0077\nEpoch 542/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0106\nEpoch 542: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0106\nEpoch 543/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0105\nEpoch 543: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0106\nEpoch 544/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0052\nEpoch 544: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0062\nEpoch 545/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0083\nEpoch 545: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0094\nEpoch 546/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0066\nEpoch 546: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0065\nEpoch 547/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0058\nEpoch 547: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0057\nEpoch 548/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0042\nEpoch 548: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0046\nEpoch 549/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0031\nEpoch 549: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0043\nEpoch 550/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0042\nEpoch 550: loss did not improve from 0.00240\n27/27 [==============================] - 1s 21ms/step - loss: 0.0040\nEpoch 551/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0038\nEpoch 551: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0036\nEpoch 552/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0041\nEpoch 552: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0041\nEpoch 553/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0037\nEpoch 553: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0035\nEpoch 554/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0039\nEpoch 554: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0037\nEpoch 555/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0036\nEpoch 555: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0036\nEpoch 556/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0030\nEpoch 556: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0040\nEpoch 557/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0035\nEpoch 557: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0035\nEpoch 558/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0043\nEpoch 558: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0040\nEpoch 559/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0042\nEpoch 559: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0041\nEpoch 560/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0043\nEpoch 560: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0043\nEpoch 561/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0033\nEpoch 561: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0032\nEpoch 562/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0035\nEpoch 562: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0033\nEpoch 563/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0036\nEpoch 563: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0041\nEpoch 564/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0037\nEpoch 564: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0037\nEpoch 565/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0040\nEpoch 565: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0038\nEpoch 566/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0034\nEpoch 566: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0038\nEpoch 567/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0040\nEpoch 567: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0040\nEpoch 568/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0036\nEpoch 568: loss did not improve from 0.00240\n27/27 [==============================] - 1s 21ms/step - loss: 0.0036\nEpoch 569/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0039\nEpoch 569: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0039\nEpoch 570/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0146\nEpoch 570: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0161\nEpoch 571/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0129\nEpoch 571: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0129\nEpoch 572/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0211\nEpoch 572: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0206\nEpoch 573/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0320\nEpoch 573: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0313\nEpoch 574/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0248\nEpoch 574: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0238\nEpoch 575/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0154\nEpoch 575: loss did not improve from 0.00240\n27/27 [==============================] - 1s 21ms/step - loss: 0.0149\nEpoch 576/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0172\nEpoch 576: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0175\nEpoch 577/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0090\nEpoch 577: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0090\nEpoch 578/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0079\nEpoch 578: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0079\nEpoch 579/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0075\nEpoch 579: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0072\nEpoch 580/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0062\nEpoch 580: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0062\nEpoch 581/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0091\nEpoch 581: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0089\nEpoch 582/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0080\nEpoch 582: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0080\nEpoch 583/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0051\nEpoch 583: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0050\nEpoch 584/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0048\nEpoch 584: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0046\nEpoch 585/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0043\nEpoch 585: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0042\nEpoch 586/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0036\nEpoch 586: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0035\nEpoch 587/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0035\nEpoch 587: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0035\nEpoch 588/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0037\nEpoch 588: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0037\nEpoch 589/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0032\nEpoch 589: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0032\nEpoch 590/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0022\nEpoch 590: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0036\nEpoch 591/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0025\nEpoch 591: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0040\nEpoch 592/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0036\nEpoch 592: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0036\nEpoch 593/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0036\nEpoch 593: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0034\nEpoch 594/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0040\nEpoch 594: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0037\nEpoch 595/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0036\nEpoch 595: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0034\nEpoch 596/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0034\nEpoch 596: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0034\nEpoch 597/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0034\nEpoch 597: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0034\nEpoch 598/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0037\nEpoch 598: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0035\nEpoch 599/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0035\nEpoch 599: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0033\nEpoch 600/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0039\nEpoch 600: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0037\nEpoch 601/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0033\nEpoch 601: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0033\nEpoch 602/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0035\nEpoch 602: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0035\nEpoch 603/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0031\nEpoch 603: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0033\nEpoch 604/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0030\nEpoch 604: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0029\nEpoch 605/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0024\nEpoch 605: loss did not improve from 0.00240\n27/27 [==============================] - 1s 21ms/step - loss: 0.0027\nEpoch 606/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0043\nEpoch 606: loss did not improve from 0.00240\n27/27 [==============================] - 1s 21ms/step - loss: 0.0041\nEpoch 607/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0033\nEpoch 607: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0032\nEpoch 608/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0031\nEpoch 608: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0030\nEpoch 609/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0032\nEpoch 609: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0031\nEpoch 610/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0031\nEpoch 610: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0031\nEpoch 611/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0034\nEpoch 611: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0032\nEpoch 612/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0035\nEpoch 612: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0033\nEpoch 613/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0038\nEpoch 613: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0036\nEpoch 614/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0032\nEpoch 614: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0032\nEpoch 615/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0034\nEpoch 615: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0032\nEpoch 616/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0028\nEpoch 616: loss did not improve from 0.00240\n27/27 [==============================] - 1s 21ms/step - loss: 0.0028\nEpoch 617/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0032\nEpoch 617: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0031\nEpoch 618/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0031\nEpoch 618: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0031\nEpoch 619/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0039\nEpoch 619: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0039\nEpoch 620/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0030\nEpoch 620: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0030\nEpoch 621/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0035\nEpoch 621: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0035\nEpoch 622/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0031\nEpoch 622: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0031\nEpoch 623/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0030\nEpoch 623: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0030\nEpoch 624/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0025\nEpoch 624: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0026\nEpoch 625/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0031\nEpoch 625: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0034\nEpoch 626/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0030\nEpoch 626: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0030\nEpoch 627/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0038\nEpoch 627: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0036\nEpoch 628/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0032\nEpoch 628: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0030\nEpoch 629/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0027\nEpoch 629: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0027\nEpoch 630/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0033\nEpoch 630: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0033\nEpoch 631/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0032\nEpoch 631: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0032\nEpoch 632/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0033\nEpoch 632: loss did not improve from 0.00240\n27/27 [==============================] - 1s 21ms/step - loss: 0.0033\nEpoch 633/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0032\nEpoch 633: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0031\nEpoch 634/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0033\nEpoch 634: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0032\nEpoch 635/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0039\nEpoch 635: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0038\nEpoch 636/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0032\nEpoch 636: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0032\nEpoch 637/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0205\nEpoch 637: loss did not improve from 0.00240\n27/27 [==============================] - 1s 21ms/step - loss: 0.0217\nEpoch 638/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0847\nEpoch 638: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0847\nEpoch 639/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0564\nEpoch 639: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0564\nEpoch 640/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0422\nEpoch 640: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0424\nEpoch 641/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0224\nEpoch 641: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0224\nEpoch 642/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0132\nEpoch 642: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0144\nEpoch 643/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0178\nEpoch 643: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0174\nEpoch 644/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0147\nEpoch 644: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0147\nEpoch 645/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0144\nEpoch 645: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0144\nEpoch 646/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0123\nEpoch 646: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0121\nEpoch 647/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0079\nEpoch 647: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0079\nEpoch 648/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0252\nEpoch 648: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0268\nEpoch 649/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0234\nEpoch 649: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0234\nEpoch 650/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0213\nEpoch 650: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0208\nEpoch 651/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0349\nEpoch 651: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0339\nEpoch 652/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.2422\nEpoch 652: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.2338\nEpoch 653/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.1550\nEpoch 653: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.1492\nEpoch 654/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0767\nEpoch 654: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0767\nEpoch 655/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0494\nEpoch 655: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0495\nEpoch 656/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0324\nEpoch 656: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0324\nEpoch 657/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0265\nEpoch 657: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0252\nEpoch 658/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0194\nEpoch 658: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0194\nEpoch 659/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0169\nEpoch 659: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0169\nEpoch 660/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0142\nEpoch 660: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0143\nEpoch 661/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0129\nEpoch 661: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0129\nEpoch 662/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0118\nEpoch 662: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0118\nEpoch 663/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0120\nEpoch 663: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0117\nEpoch 664/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0158\nEpoch 664: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0158\nEpoch 665/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0190\nEpoch 665: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0190\nEpoch 666/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0184\nEpoch 666: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0176\nEpoch 667/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0118\nEpoch 667: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0132\nEpoch 668/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0129\nEpoch 668: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0126\nEpoch 669/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0090\nEpoch 669: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0090\nEpoch 670/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0090\nEpoch 670: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0090\nEpoch 671/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0070\nEpoch 671: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0068\nEpoch 672/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0069\nEpoch 672: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0069\nEpoch 673/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0060\nEpoch 673: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0060\nEpoch 674/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0068\nEpoch 674: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0068\nEpoch 675/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0068\nEpoch 675: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0068\nEpoch 676/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0054\nEpoch 676: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0054\nEpoch 677/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0051\nEpoch 677: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0051\nEpoch 678/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0047\nEpoch 678: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0046\nEpoch 679/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0057\nEpoch 679: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0057\nEpoch 680/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0053\nEpoch 680: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0054\nEpoch 681/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0044\nEpoch 681: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0043\nEpoch 682/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0055\nEpoch 682: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0052\nEpoch 683/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0046\nEpoch 683: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0046\nEpoch 684/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0042\nEpoch 684: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0042\nEpoch 685/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0046\nEpoch 685: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0046\nEpoch 686/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0040\nEpoch 686: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0040\nEpoch 687/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0045\nEpoch 687: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0045\nEpoch 688/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0041\nEpoch 688: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0040\nEpoch 689/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0046\nEpoch 689: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0046\nEpoch 690/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0058\nEpoch 690: loss did not improve from 0.00240\n27/27 [==============================] - 1s 21ms/step - loss: 0.0058\nEpoch 691/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0050\nEpoch 691: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0050\nEpoch 692/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0043\nEpoch 692: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0041\nEpoch 693/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0036\nEpoch 693: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0036\nEpoch 694/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0043\nEpoch 694: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0042\nEpoch 695/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0041\nEpoch 695: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0041\nEpoch 696/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0033\nEpoch 696: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0034\nEpoch 697/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0143\nEpoch 697: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0142\nEpoch 698/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0304\nEpoch 698: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0301\nEpoch 699/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0192\nEpoch 699: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0192\nEpoch 700/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0227\nEpoch 700: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0218\nEpoch 701/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0134\nEpoch 701: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0127\nEpoch 702/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0086\nEpoch 702: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0086\nEpoch 703/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0054\nEpoch 703: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0054\nEpoch 704/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0052\nEpoch 704: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0051\nEpoch 705/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0051\nEpoch 705: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0051\nEpoch 706/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0049\nEpoch 706: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0049\nEpoch 707/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0043\nEpoch 707: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0043\nEpoch 708/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0048\nEpoch 708: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0048\nEpoch 709/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0040\nEpoch 709: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0040\nEpoch 710/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0036\nEpoch 710: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0035\nEpoch 711/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0046\nEpoch 711: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0043\nEpoch 712/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0053\nEpoch 712: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0052\nEpoch 713/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0038\nEpoch 713: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0038\nEpoch 714/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0037\nEpoch 714: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0037\nEpoch 715/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0042\nEpoch 715: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0042\nEpoch 716/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0034\nEpoch 716: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0042\nEpoch 717/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0038\nEpoch 717: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0039\nEpoch 718/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0035\nEpoch 718: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0033\nEpoch 719/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0045\nEpoch 719: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0045\nEpoch 720/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0037\nEpoch 720: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0036\nEpoch 721/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0028\nEpoch 721: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0037\nEpoch 722/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0043\nEpoch 722: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0042\nEpoch 723/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0042\nEpoch 723: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0042\nEpoch 724/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0036\nEpoch 724: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0035\nEpoch 725/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0035\nEpoch 725: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0034\nEpoch 726/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0035\nEpoch 726: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0038\nEpoch 727/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0038\nEpoch 727: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0036\nEpoch 728/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0039\nEpoch 728: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0039\nEpoch 729/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0042\nEpoch 729: loss did not improve from 0.00240\n27/27 [==============================] - 1s 24ms/step - loss: 0.0040\nEpoch 730/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0030\nEpoch 730: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0039\nEpoch 731/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0042\nEpoch 731: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0040\nEpoch 732/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0038\nEpoch 732: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0037\nEpoch 733/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0035\nEpoch 733: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0034\nEpoch 734/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0019\nEpoch 734: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0030\nEpoch 735/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0040\nEpoch 735: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0040\nEpoch 736/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0034\nEpoch 736: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0032\nEpoch 737/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0039\nEpoch 737: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0039\nEpoch 738/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0041\nEpoch 738: loss did not improve from 0.00240\n27/27 [==============================] - 1s 21ms/step - loss: 0.0039\nEpoch 739/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0041\nEpoch 739: loss did not improve from 0.00240\n27/27 [==============================] - 1s 21ms/step - loss: 0.0039\nEpoch 740/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0033\nEpoch 740: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0033\nEpoch 741/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0033\nEpoch 741: loss did not improve from 0.00240\n27/27 [==============================] - 1s 21ms/step - loss: 0.0031\nEpoch 742/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0040\nEpoch 742: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0039\nEpoch 743/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0035\nEpoch 743: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0034\nEpoch 744/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0034\nEpoch 744: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0035\nEpoch 745/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0040\nEpoch 745: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0040\nEpoch 746/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0040\nEpoch 746: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0040\nEpoch 747/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0035\nEpoch 747: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0037\nEpoch 748/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0513\nEpoch 748: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0513\nEpoch 749/1000\n27/27 [==============================] - ETA: 0s - loss: 0.1126\nEpoch 749: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.1126\nEpoch 750/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0597\nEpoch 750: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0616\nEpoch 751/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0352\nEpoch 751: loss did not improve from 0.00240\n27/27 [==============================] - 1s 21ms/step - loss: 0.0345\nEpoch 752/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0298\nEpoch 752: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0298\nEpoch 753/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0323\nEpoch 753: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0323\nEpoch 754/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0343\nEpoch 754: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0343\nEpoch 755/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0263\nEpoch 755: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0263\nEpoch 756/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0218\nEpoch 756: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0231\nEpoch 757/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0162\nEpoch 757: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0169\nEpoch 758/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0141\nEpoch 758: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0141\nEpoch 759/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0109\nEpoch 759: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0108\nEpoch 760/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0105\nEpoch 760: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0105\nEpoch 761/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0090\nEpoch 761: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0090\nEpoch 762/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0101\nEpoch 762: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0099\nEpoch 763/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0078\nEpoch 763: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0076\nEpoch 764/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0066\nEpoch 764: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0062\nEpoch 765/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0056\nEpoch 765: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0055\nEpoch 766/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0053\nEpoch 766: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0054\nEpoch 767/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0033\nEpoch 767: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0060\nEpoch 768/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0047\nEpoch 768: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0047\nEpoch 769/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0047\nEpoch 769: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0046\nEpoch 770/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0060\nEpoch 770: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0056\nEpoch 771/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0045\nEpoch 771: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0045\nEpoch 772/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0045\nEpoch 772: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0045\nEpoch 773/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0047\nEpoch 773: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0047\nEpoch 774/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0064\nEpoch 774: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0064\nEpoch 775/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0056\nEpoch 775: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0056\nEpoch 776/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0050\nEpoch 776: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0061\nEpoch 777/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0073\nEpoch 777: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0073\nEpoch 778/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0347\nEpoch 778: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0338\nEpoch 779/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0233\nEpoch 779: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0243\nEpoch 780/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0141\nEpoch 780: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0134\nEpoch 781/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0075\nEpoch 781: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0076\nEpoch 782/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0070\nEpoch 782: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0068\nEpoch 783/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0057\nEpoch 783: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0057\nEpoch 784/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0153\nEpoch 784: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0153\nEpoch 785/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0176\nEpoch 785: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0176\nEpoch 786/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0089\nEpoch 786: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0099\nEpoch 787/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0108\nEpoch 787: loss did not improve from 0.00240\n27/27 [==============================] - 1s 21ms/step - loss: 0.0105\nEpoch 788/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0111\nEpoch 788: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0106\nEpoch 789/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0078\nEpoch 789: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0077\nEpoch 790/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0057\nEpoch 790: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0057\nEpoch 791/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0049\nEpoch 791: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0047\nEpoch 792/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0044\nEpoch 792: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0044\nEpoch 793/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0052\nEpoch 793: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0049\nEpoch 794/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0054\nEpoch 794: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0053\nEpoch 795/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0087\nEpoch 795: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0091\nEpoch 796/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0065\nEpoch 796: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0063\nEpoch 797/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0099\nEpoch 797: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0099\nEpoch 798/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0119\nEpoch 798: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0119\nEpoch 799/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0089\nEpoch 799: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0092\nEpoch 800/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0049\nEpoch 800: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0049\nEpoch 801/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0045\nEpoch 801: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0043\nEpoch 802/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0038\nEpoch 802: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0038\nEpoch 803/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0032\nEpoch 803: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0032\nEpoch 804/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0041\nEpoch 804: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0041\nEpoch 805/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0039\nEpoch 805: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0039\nEpoch 806/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0033\nEpoch 806: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0033\nEpoch 807/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0035\nEpoch 807: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0034\nEpoch 808/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0038\nEpoch 808: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0036\nEpoch 809/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0037\nEpoch 809: loss did not improve from 0.00240\n27/27 [==============================] - 1s 21ms/step - loss: 0.0035\nEpoch 810/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0039\nEpoch 810: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0038\nEpoch 811/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0039\nEpoch 811: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0037\nEpoch 812/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0034\nEpoch 812: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0033\nEpoch 813/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0034\nEpoch 813: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0032\nEpoch 814/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0027\nEpoch 814: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0027\nEpoch 815/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0034\nEpoch 815: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0033\nEpoch 816/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0039\nEpoch 816: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0037\nEpoch 817/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0024\nEpoch 817: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0037\nEpoch 818/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0031\nEpoch 818: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0030\nEpoch 819/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0030\nEpoch 819: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0029\nEpoch 820/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0028\nEpoch 820: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0027\nEpoch 821/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0042\nEpoch 821: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0040\nEpoch 822/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0037\nEpoch 822: loss did not improve from 0.00240\n27/27 [==============================] - 1s 24ms/step - loss: 0.0037\nEpoch 823/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0050\nEpoch 823: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0050\nEpoch 824/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0096\nEpoch 824: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0109\nEpoch 825/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0091\nEpoch 825: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0089\nEpoch 826/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0091\nEpoch 826: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0095\nEpoch 827/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0117\nEpoch 827: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0113\nEpoch 828/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0082\nEpoch 828: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0097\nEpoch 829/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0593\nEpoch 829: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0593\nEpoch 830/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0227\nEpoch 830: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0227\nEpoch 831/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0164\nEpoch 831: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0163\nEpoch 832/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0125\nEpoch 832: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0119\nEpoch 833/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0069\nEpoch 833: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0082\nEpoch 834/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0064\nEpoch 834: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0064\nEpoch 835/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0066\nEpoch 835: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0064\nEpoch 836/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0049\nEpoch 836: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0062\nEpoch 837/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0041\nEpoch 837: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0056\nEpoch 838/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0050\nEpoch 838: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0050\nEpoch 839/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0037\nEpoch 839: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0048\nEpoch 840/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0043\nEpoch 840: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0046\nEpoch 841/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0040\nEpoch 841: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0040\nEpoch 842/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0045\nEpoch 842: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0045\nEpoch 843/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0041\nEpoch 843: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0041\nEpoch 844/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0048\nEpoch 844: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0049\nEpoch 845/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0060\nEpoch 845: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0060\nEpoch 846/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0150\nEpoch 846: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0150\nEpoch 847/1000\n27/27 [==============================] - ETA: 0s - loss: 0.1220\nEpoch 847: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.1220\nEpoch 848/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0620\nEpoch 848: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0620\nEpoch 849/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0382\nEpoch 849: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0409\nEpoch 850/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0266\nEpoch 850: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0260\nEpoch 851/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0175\nEpoch 851: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0175\nEpoch 852/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0139\nEpoch 852: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0134\nEpoch 853/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0129\nEpoch 853: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0129\nEpoch 854/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0103\nEpoch 854: loss did not improve from 0.00240\n27/27 [==============================] - 1s 21ms/step - loss: 0.0098\nEpoch 855/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0105\nEpoch 855: loss did not improve from 0.00240\n27/27 [==============================] - 1s 21ms/step - loss: 0.0100\nEpoch 856/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0080\nEpoch 856: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0079\nEpoch 857/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0075\nEpoch 857: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0073\nEpoch 858/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0090\nEpoch 858: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0091\nEpoch 859/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0073\nEpoch 859: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0072\nEpoch 860/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0056\nEpoch 860: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0054\nEpoch 861/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0058\nEpoch 861: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0055\nEpoch 862/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0044\nEpoch 862: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0044\nEpoch 863/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0049\nEpoch 863: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0046\nEpoch 864/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0044\nEpoch 864: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0049\nEpoch 865/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0048\nEpoch 865: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0048\nEpoch 866/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0051\nEpoch 866: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0050\nEpoch 867/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0036\nEpoch 867: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0036\nEpoch 868/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0047\nEpoch 868: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0047\nEpoch 869/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0040\nEpoch 869: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0040\nEpoch 870/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0039\nEpoch 870: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0039\nEpoch 871/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0042\nEpoch 871: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0041\nEpoch 872/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0039\nEpoch 872: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0038\nEpoch 873/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0043\nEpoch 873: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0042\nEpoch 874/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0032\nEpoch 874: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0032\nEpoch 875/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0028\nEpoch 875: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0036\nEpoch 876/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0037\nEpoch 876: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0037\nEpoch 877/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0042\nEpoch 877: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0040\nEpoch 878/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0032\nEpoch 878: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0032\nEpoch 879/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0040\nEpoch 879: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0040\nEpoch 880/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0038\nEpoch 880: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0038\nEpoch 881/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0036\nEpoch 881: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0036\nEpoch 882/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0034\nEpoch 882: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0033\nEpoch 883/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0032\nEpoch 883: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0032\nEpoch 884/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0034\nEpoch 884: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0034\nEpoch 885/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0034\nEpoch 885: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0032\nEpoch 886/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0028\nEpoch 886: loss did not improve from 0.00240\n27/27 [==============================] - 1s 24ms/step - loss: 0.0027\nEpoch 887/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0041\nEpoch 887: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0038\nEpoch 888/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0035\nEpoch 888: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0035\nEpoch 889/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0035\nEpoch 889: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0033\nEpoch 890/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0030\nEpoch 890: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0030\nEpoch 891/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0025\nEpoch 891: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0036\nEpoch 892/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0031\nEpoch 892: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0030\nEpoch 893/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0035\nEpoch 893: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0034\nEpoch 894/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0022\nEpoch 894: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0031\nEpoch 895/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0035\nEpoch 895: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0035\nEpoch 896/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0037\nEpoch 896: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0037\nEpoch 897/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0023\nEpoch 897: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0036\nEpoch 898/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0037\nEpoch 898: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0036\nEpoch 899/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0290\nEpoch 899: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0296\nEpoch 900/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0428\nEpoch 900: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0428\nEpoch 901/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0207\nEpoch 901: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0208\nEpoch 902/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0168\nEpoch 902: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0164\nEpoch 903/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0130\nEpoch 903: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0140\nEpoch 904/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0284\nEpoch 904: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0294\nEpoch 905/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0239\nEpoch 905: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0239\nEpoch 906/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0125\nEpoch 906: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0123\nEpoch 907/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0113\nEpoch 907: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0110\nEpoch 908/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0064\nEpoch 908: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0064\nEpoch 909/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0066\nEpoch 909: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0064\nEpoch 910/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0077\nEpoch 910: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0077\nEpoch 911/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0058\nEpoch 911: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0058\nEpoch 912/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0041\nEpoch 912: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0055\nEpoch 913/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0048\nEpoch 913: loss did not improve from 0.00240\n27/27 [==============================] - 1s 21ms/step - loss: 0.0046\nEpoch 914/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0077\nEpoch 914: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0077\nEpoch 915/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0056\nEpoch 915: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0054\nEpoch 916/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0183\nEpoch 916: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0187\nEpoch 917/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0415\nEpoch 917: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0419\nEpoch 918/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0250\nEpoch 918: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0245\nEpoch 919/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0158\nEpoch 919: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0155\nEpoch 920/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0114\nEpoch 920: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0119\nEpoch 921/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0172\nEpoch 921: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0170\nEpoch 922/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0076\nEpoch 922: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0079\nEpoch 923/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0059\nEpoch 923: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0058\nEpoch 924/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0041\nEpoch 924: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0043\nEpoch 925/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0040\nEpoch 925: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0039\nEpoch 926/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0037\nEpoch 926: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0036\nEpoch 927/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0028\nEpoch 927: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0036\nEpoch 928/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0038\nEpoch 928: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0038\nEpoch 929/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0038\nEpoch 929: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0036\nEpoch 930/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0019\nEpoch 930: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0037\nEpoch 931/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0032\nEpoch 931: loss did not improve from 0.00240\n27/27 [==============================] - 1s 24ms/step - loss: 0.0032\nEpoch 932/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0037\nEpoch 932: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0037\nEpoch 933/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0033\nEpoch 933: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0033\nEpoch 934/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0035\nEpoch 934: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0035\nEpoch 935/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0035\nEpoch 935: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0035\nEpoch 936/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0032\nEpoch 936: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0034\nEpoch 937/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0031\nEpoch 937: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0031\nEpoch 938/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0034\nEpoch 938: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0034\nEpoch 939/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0035\nEpoch 939: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0034\nEpoch 940/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0030\nEpoch 940: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0030\nEpoch 941/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0039\nEpoch 941: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0037\nEpoch 942/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0031\nEpoch 942: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0031\nEpoch 943/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0023\nEpoch 943: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0033\nEpoch 944/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0027\nEpoch 944: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0027\nEpoch 945/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0034\nEpoch 945: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0034\nEpoch 946/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0031\nEpoch 946: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0030\nEpoch 947/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0037\nEpoch 947: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0035\nEpoch 948/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0031\nEpoch 948: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0029\nEpoch 949/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0037\nEpoch 949: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0037\nEpoch 950/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0037\nEpoch 950: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0036\nEpoch 951/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0032\nEpoch 951: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0032\nEpoch 952/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0037\nEpoch 952: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0034\nEpoch 953/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0031\nEpoch 953: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0031\nEpoch 954/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0032\nEpoch 954: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0031\nEpoch 955/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0026\nEpoch 955: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0028\nEpoch 956/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0026\nEpoch 956: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0028\nEpoch 957/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0034\nEpoch 957: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0032\nEpoch 958/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0040\nEpoch 958: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0038\nEpoch 959/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0033\nEpoch 959: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0031\nEpoch 960/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0030\nEpoch 960: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0030\nEpoch 961/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0032\nEpoch 961: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0031\nEpoch 962/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0037\nEpoch 962: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0037\nEpoch 963/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0034\nEpoch 963: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0032\nEpoch 964/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0030\nEpoch 964: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0030\nEpoch 965/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0031\nEpoch 965: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0031\nEpoch 966/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0027\nEpoch 966: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0027\nEpoch 967/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0031\nEpoch 967: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0031\nEpoch 968/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0029\nEpoch 968: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0029\nEpoch 969/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0037\nEpoch 969: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0036\nEpoch 970/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0034\nEpoch 970: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0034\nEpoch 971/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0030\nEpoch 971: loss did not improve from 0.00240\n27/27 [==============================] - 1s 24ms/step - loss: 0.0029\nEpoch 972/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0033\nEpoch 972: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0033\nEpoch 973/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0030\nEpoch 973: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0030\nEpoch 974/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0026\nEpoch 974: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0025\nEpoch 975/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0031\nEpoch 975: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0030\nEpoch 976/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0028\nEpoch 976: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0027\nEpoch 977/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0030\nEpoch 977: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0029\nEpoch 978/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0026\nEpoch 978: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0026\nEpoch 979/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0028\nEpoch 979: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0028\nEpoch 980/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0038\nEpoch 980: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0037\nEpoch 981/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0029\nEpoch 981: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0029\nEpoch 982/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0036\nEpoch 982: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0035\nEpoch 983/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0027\nEpoch 983: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0027\nEpoch 984/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0032\nEpoch 984: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0032\nEpoch 985/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0020\nEpoch 985: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0029\nEpoch 986/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0030\nEpoch 986: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0030\nEpoch 987/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0029\nEpoch 987: loss did not improve from 0.00240\n27/27 [==============================] - 1s 24ms/step - loss: 0.0029\nEpoch 988/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0021\nEpoch 988: loss did not improve from 0.00240\n27/27 [==============================] - 1s 21ms/step - loss: 0.0030\nEpoch 989/1000\n25/27 [==========================&gt;...] - ETA: 0s - loss: 0.0184\nEpoch 989: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0207\nEpoch 990/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0606\nEpoch 990: loss did not improve from 0.00240\n27/27 [==============================] - 1s 24ms/step - loss: 0.0606\nEpoch 991/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0384\nEpoch 991: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0384\nEpoch 992/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0329\nEpoch 992: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0322\nEpoch 993/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0219\nEpoch 993: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0219\nEpoch 994/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0124\nEpoch 994: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0124\nEpoch 995/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0123\nEpoch 995: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0123\nEpoch 996/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0087\nEpoch 996: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0087\nEpoch 997/1000\n26/27 [===========================&gt;..] - ETA: 0s - loss: 0.0092\nEpoch 997: loss did not improve from 0.00240\n27/27 [==============================] - 1s 23ms/step - loss: 0.0090\nEpoch 998/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0125\nEpoch 998: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0125\nEpoch 999/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0128\nEpoch 999: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0128\nEpoch 1000/1000\n27/27 [==============================] - ETA: 0s - loss: 0.0084\nEpoch 1000: loss did not improve from 0.00240\n27/27 [==============================] - 1s 22ms/step - loss: 0.0084\n\n\nВы увидите разные результаты из-за стохастической природы модели и из-за того, что трудно подобрать случайное начальное число для моделей LSTM, чтобы получить 100% воспроизводимые результаты. Это не касается этой генеративной модели.\nПосле запуска примера у вас должно быть несколько файлов контрольных точек веса в локальном каталоге.\nВы можете удалить их все, кроме одного с наименьшим значением потери. Например, когда я запускал этот пример, ниже был контрольный пункт с наименьшей потерей, которую я достиг.\nweights-improvement-456-0.0024.hdf5\n\n\nCode\nprint(history.history.keys())\nmodel.metrics_names\n\n\n['loss']\n\n\n\n\nCode\nplt.plot(history.history['loss'])\n\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper right')\nplt.show()\n\n\n\n\n\nFigure 1: График потерь на обучении\n\n\n\n\n#Генерация текста с помощью сети LSTM\nГенерация текста с использованием обученной сети LSTM относительно проста.\nВо-первых, мы загружаем данные и определяем сеть точно таким же образом, за исключением того, что веса сети загружаются из файла контрольных точек, и сеть не нуждается в обучении.\n\n\nCode\n# load the network weights\nfilename = \"/content/drive/MyDrive/Проекты/ML/Pets/rnn_mario_level_builder/data/weights-improvement-456-0.0024.hdf5\"\nmodel.load_weights(filename)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n\nКроме того, при подготовке сопоставления уникальных символов с целыми числами мы также должны создать обратное отображение, которое мы можем использовать для преобразования целых чисел обратно в символы, чтобы мы могли понять предсказания.\n\n\nCode\nint_to_char = dict((i, c) for i, c in enumerate(chars))\n\n\nНаконец, нам нужно делать прогнозы.\nПростейший способ использования модели Keras LSTM для прогнозирования - сначала начать с последовательности начальных чисел в качестве входных данных, сгенерировать следующий символ, затем обновить последовательность начальных чисел, чтобы добавить сгенерированный символ в конце, и обрезать первый символ. Этот процесс повторяется до тех пор, пока мы хотим предсказать новые символы (например, последовательность длиной 1000 символов).\nМы можем выбрать случайный шаблон ввода в качестве нашей начальной последовательности, а затем распечатать сгенерированные символы по мере их генерации.\n\n\nCode\nimport sys\n# pick a random seed\nstart = np.random.randint(0, len(dataX)-1)\npattern = dataX[start]\n\nprint(\"Seed:\")\nprint(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n\n# generate characters\nfor i in range(500):\n    x = np.reshape(pattern, (1, len(pattern), 1))\n    x = x / float(n_vocab)\n    prediction = model.predict(x, verbose=0)\n    index = np.argmax(prediction)\n    result = int_to_char[index]\n    seq_in = [int_to_char[value] for value in pattern]\n    sys.stdout.write(result)\n    pattern.append(index)\n    pattern = pattern[1:len(pattern)]\n\nprint(\"\\nDone.\")\n\n\nSeed:\n\" -------#GG\n-----------##GG\n----------###GG\n---------####GG\n---------####GG\n---------------\n---------------\n---------####GG\n----------###GG\n-----------##GG\n----- \"\n-------#GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-----------PPGG\n-----------PPGG\n-------------GG\n-------------GG\n-------------GG\n---------=---GG\n---------=---GG\n---------?---GG\n---------=---GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-------------GG\n-----------PPGG\n-----------PPGG\n------------#GG\n-----------##GG\n----------###GG\n---------####GG\n--------#####GG\n-------######GG\n------#######GG\n-----########GG\n-----####\nDone."
  },
  {
    "objectID": "posts/post-with-code/Demo  seamless-m4t-large.html",
    "href": "posts/post-with-code/Demo  seamless-m4t-large.html",
    "title": "Пример работы с seamless-m4t-large от Мета",
    "section": "",
    "text": "Meta выпустила крутой нейропереводчик с поддержкой 100 языков. Он спокойно может перегнать текст в речь и наоборот. Из речи в речь тоже умеет.\nИнтонации переданы неплохо. Это не стандартный робот-тараторщик, а вполне приятный искусственный интеллект с нормальным голосом.\nМодель SeamlessM4T в настоящее время доступна через пакет seamless_communication\nПриступим к установке\n\n\nCode\n!pip install fairseq2==0.1\n\n\nCollecting fairseq2==0.1\n  Downloading fairseq2-0.1.0-py3-none-any.whl (162 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 162.3/162.3 kB 3.4 MB/s eta 0:00:00\nCollecting fairseq2n==0.1.0 (from fairseq2==0.1)\n  Downloading fairseq2n-0.1.0-cp310-cp310-manylinux_2_17_x86_64.whl (2.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 11.5 MB/s eta 0:00:00\nCollecting jiwer~=3.0 (from fairseq2==0.1)\n  Downloading jiwer-3.0.2-py3-none-any.whl (21 kB)\nRequirement already satisfied: numpy~=1.23 in /usr/local/lib/python3.10/dist-packages (from fairseq2==0.1) (1.23.5)\nCollecting overrides~=7.3 (from fairseq2==0.1)\n  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\nRequirement already satisfied: packaging~=23.1 in /usr/local/lib/python3.10/dist-packages (from fairseq2==0.1) (23.1)\nRequirement already satisfied: pyyaml~=6.0 in /usr/local/lib/python3.10/dist-packages (from fairseq2==0.1) (6.0.1)\nCollecting sacrebleu~=2.3 (from fairseq2==0.1)\n  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 118.9/118.9 kB 12.6 MB/s eta 0:00:00\nRequirement already satisfied: torch&gt;=1.12.1 in /usr/local/lib/python3.10/dist-packages (from fairseq2==0.1) (2.0.1+cu118)\nCollecting torcheval~=0.0.6 (from fairseq2==0.1)\n  Downloading torcheval-0.0.6-py3-none-any.whl (158 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 158.4/158.4 kB 15.4 MB/s eta 0:00:00\nCollecting tbb==2021.8 (from fairseq2n==0.1.0-&gt;fairseq2==0.1)\n  Downloading tbb-2021.8.0-py2.py3-none-manylinux1_x86_64.whl (4.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.0/4.0 MB 21.8 MB/s eta 0:00:00\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.12.1-&gt;fairseq2==0.1) (3.12.2)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.12.1-&gt;fairseq2==0.1) (4.7.1)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.12.1-&gt;fairseq2==0.1) (1.12)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.12.1-&gt;fairseq2==0.1) (3.1)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.12.1-&gt;fairseq2==0.1) (3.1.2)\nRequirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.12.1-&gt;fairseq2==0.1) (2.0.0)\nRequirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-&gt;torch&gt;=1.12.1-&gt;fairseq2==0.1) (3.27.2)\nRequirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-&gt;torch&gt;=1.12.1-&gt;fairseq2==0.1) (16.0.6)\nRequirement already satisfied: click&lt;9.0.0,&gt;=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer~=3.0-&gt;fairseq2==0.1) (8.1.7)\nCollecting rapidfuzz==2.13.7 (from jiwer~=3.0-&gt;fairseq2==0.1)\n  Downloading rapidfuzz-2.13.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 32.0 MB/s eta 0:00:00\nCollecting portalocker (from sacrebleu~=2.3-&gt;fairseq2==0.1)\n  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu~=2.3-&gt;fairseq2==0.1) (2023.6.3)\nRequirement already satisfied: tabulate&gt;=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu~=2.3-&gt;fairseq2==0.1) (0.9.0)\nCollecting colorama (from sacrebleu~=2.3-&gt;fairseq2==0.1)\n  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu~=2.3-&gt;fairseq2==0.1) (4.9.3)\nCollecting torchtnt&gt;=0.0.5 (from torcheval~=0.0.6-&gt;fairseq2==0.1)\n  Downloading torchtnt-0.2.0-py3-none-any.whl (111 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 111.0/111.0 kB 14.9 MB/s eta 0:00:00\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torchtnt&gt;=0.0.5-&gt;torcheval~=0.0.6-&gt;fairseq2==0.1) (2023.6.0)\nRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from torchtnt&gt;=0.0.5-&gt;torcheval~=0.0.6-&gt;fairseq2==0.1) (2.12.3)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from torchtnt&gt;=0.0.5-&gt;torcheval~=0.0.6-&gt;fairseq2==0.1) (5.9.5)\nCollecting pyre-extensions (from torchtnt&gt;=0.0.5-&gt;torcheval~=0.0.6-&gt;fairseq2==0.1)\n  Downloading pyre_extensions-0.0.30-py3-none-any.whl (12 kB)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from torchtnt&gt;=0.0.5-&gt;torcheval~=0.0.6-&gt;fairseq2==0.1) (67.7.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtnt&gt;=0.0.5-&gt;torcheval~=0.0.6-&gt;fairseq2==0.1) (4.66.1)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;torch&gt;=1.12.1-&gt;fairseq2==0.1) (2.1.3)\nRequirement already satisfied: mpmath&gt;=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-&gt;torch&gt;=1.12.1-&gt;fairseq2==0.1) (1.3.0)\nCollecting typing-inspect (from pyre-extensions-&gt;torchtnt&gt;=0.0.5-&gt;torcheval~=0.0.6-&gt;fairseq2==0.1)\n  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\nRequirement already satisfied: absl-py&gt;=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard-&gt;torchtnt&gt;=0.0.5-&gt;torcheval~=0.0.6-&gt;fairseq2==0.1) (1.4.0)\nRequirement already satisfied: grpcio&gt;=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard-&gt;torchtnt&gt;=0.0.5-&gt;torcheval~=0.0.6-&gt;fairseq2==0.1) (1.57.0)\nRequirement already satisfied: google-auth&lt;3,&gt;=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard-&gt;torchtnt&gt;=0.0.5-&gt;torcheval~=0.0.6-&gt;fairseq2==0.1) (2.17.3)\nRequirement already satisfied: google-auth-oauthlib&lt;1.1,&gt;=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard-&gt;torchtnt&gt;=0.0.5-&gt;torcheval~=0.0.6-&gt;fairseq2==0.1) (1.0.0)\nRequirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard-&gt;torchtnt&gt;=0.0.5-&gt;torcheval~=0.0.6-&gt;fairseq2==0.1) (3.4.4)\nRequirement already satisfied: protobuf&gt;=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard-&gt;torchtnt&gt;=0.0.5-&gt;torcheval~=0.0.6-&gt;fairseq2==0.1) (3.20.3)\nRequirement already satisfied: requests&lt;3,&gt;=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard-&gt;torchtnt&gt;=0.0.5-&gt;torcheval~=0.0.6-&gt;fairseq2==0.1) (2.31.0)\nRequirement already satisfied: tensorboard-data-server&lt;0.8.0,&gt;=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard-&gt;torchtnt&gt;=0.0.5-&gt;torcheval~=0.0.6-&gt;fairseq2==0.1) (0.7.1)\nRequirement already satisfied: werkzeug&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard-&gt;torchtnt&gt;=0.0.5-&gt;torcheval~=0.0.6-&gt;fairseq2==0.1) (2.3.7)\nRequirement already satisfied: wheel&gt;=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard-&gt;torchtnt&gt;=0.0.5-&gt;torcheval~=0.0.6-&gt;fairseq2==0.1) (0.41.1)\nRequirement already satisfied: cachetools&lt;6.0,&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard-&gt;torchtnt&gt;=0.0.5-&gt;torcheval~=0.0.6-&gt;fairseq2==0.1) (5.3.1)\nRequirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard-&gt;torchtnt&gt;=0.0.5-&gt;torcheval~=0.0.6-&gt;fairseq2==0.1) (0.3.0)\nRequirement already satisfied: six&gt;=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard-&gt;torchtnt&gt;=0.0.5-&gt;torcheval~=0.0.6-&gt;fairseq2==0.1) (1.16.0)\nRequirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard-&gt;torchtnt&gt;=0.0.5-&gt;torcheval~=0.0.6-&gt;fairseq2==0.1) (4.9)\nRequirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib&lt;1.1,&gt;=0.5-&gt;tensorboard-&gt;torchtnt&gt;=0.0.5-&gt;torcheval~=0.0.6-&gt;fairseq2==0.1) (1.3.1)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard-&gt;torchtnt&gt;=0.0.5-&gt;torcheval~=0.0.6-&gt;fairseq2==0.1) (3.2.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard-&gt;torchtnt&gt;=0.0.5-&gt;torcheval~=0.0.6-&gt;fairseq2==0.1) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard-&gt;torchtnt&gt;=0.0.5-&gt;torcheval~=0.0.6-&gt;fairseq2==0.1) (2.0.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard-&gt;torchtnt&gt;=0.0.5-&gt;torcheval~=0.0.6-&gt;fairseq2==0.1) (2023.7.22)\nCollecting mypy-extensions&gt;=0.3.0 (from typing-inspect-&gt;pyre-extensions-&gt;torchtnt&gt;=0.0.5-&gt;torcheval~=0.0.6-&gt;fairseq2==0.1)\n  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\nRequirement already satisfied: pyasn1&lt;0.6.0,&gt;=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard-&gt;torchtnt&gt;=0.0.5-&gt;torcheval~=0.0.6-&gt;fairseq2==0.1) (0.5.0)\nRequirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;1.1,&gt;=0.5-&gt;tensorboard-&gt;torchtnt&gt;=0.0.5-&gt;torcheval~=0.0.6-&gt;fairseq2==0.1) (3.2.2)\nInstalling collected packages: tbb, rapidfuzz, portalocker, overrides, mypy-extensions, colorama, typing-inspect, sacrebleu, jiwer, pyre-extensions, torchtnt, torcheval, fairseq2n, fairseq2\n  Attempting uninstall: tbb\n    Found existing installation: tbb 2021.10.0\n    Uninstalling tbb-2021.10.0:\n      Successfully uninstalled tbb-2021.10.0\nSuccessfully installed colorama-0.4.6 fairseq2-0.1.0 fairseq2n-0.1.0 jiwer-3.0.2 mypy-extensions-1.0.0 overrides-7.4.0 portalocker-2.7.0 pyre-extensions-0.0.30 rapidfuzz-2.13.7 sacrebleu-2.3.1 tbb-2021.8.0 torcheval-0.0.6 torchtnt-0.2.0 typing-inspect-0.9.0\n\n\nСкачаем установщик с гитхаба\n\n\nCode\n!wget https://github.com/facebookresearch/seamless_communication/archive/refs/heads/main.zip && unzip main.zip\n\n\n--2023-08-22 16:14:48--  https://github.com/facebookresearch/seamless_communication/archive/refs/heads/main.zip\nResolving github.com (github.com)... 140.82.112.3\nConnecting to github.com (github.com)|140.82.112.3|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://codeload.github.com/facebookresearch/seamless_communication/zip/refs/heads/main [following]\n--2023-08-22 16:14:48--  https://codeload.github.com/facebookresearch/seamless_communication/zip/refs/heads/main\nResolving codeload.github.com (codeload.github.com)... 140.82.114.9\nConnecting to codeload.github.com (codeload.github.com)|140.82.114.9|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [application/zip]\nSaving to: ‘main.zip’\n\nmain.zip                [ &lt;=&gt;                ] 268.61K  --.-KB/s    in 0.05s   \n\n2023-08-22 16:14:48 (5.26 MB/s) - ‘main.zip’ saved [275053]\n\nArchive:  main.zip\n01c1042841f9bce66902eb2c7512dbdd71d42112\n   creating: seamless_communication-main/\n  inflating: seamless_communication-main/.gitignore  \n  inflating: seamless_communication-main/.pre-commit-config.yaml  \n  inflating: seamless_communication-main/CODE_OF_CONDUCT.md  \n  inflating: seamless_communication-main/CONTRIBUTING.md  \n  inflating: seamless_communication-main/LICENSE  \n  inflating: seamless_communication-main/README.md  \n   creating: seamless_communication-main/docs/\n   creating: seamless_communication-main/docs/m4t/\n  inflating: seamless_communication-main/docs/m4t/eval_README.md  \n  inflating: seamless_communication-main/docs/m4t/on_device_README.md  \n  inflating: seamless_communication-main/docs/m4t/seamless_align_README.md  \n extracting: seamless_communication-main/requirements.txt  \n   creating: seamless_communication-main/scripts/\n   creating: seamless_communication-main/scripts/m4t/\n   creating: seamless_communication-main/scripts/m4t/finetune/\n  inflating: seamless_communication-main/scripts/m4t/finetune/README.md  \n extracting: seamless_communication-main/scripts/m4t/finetune/__init__.py  \n  inflating: seamless_communication-main/scripts/m4t/finetune/dataloader.py  \n  inflating: seamless_communication-main/scripts/m4t/finetune/dataset.py  \n  inflating: seamless_communication-main/scripts/m4t/finetune/dist_utils.py  \n  inflating: seamless_communication-main/scripts/m4t/finetune/finetune.py  \n  inflating: seamless_communication-main/scripts/m4t/finetune/trainer.py  \n   creating: seamless_communication-main/scripts/m4t/predict/\n  inflating: seamless_communication-main/scripts/m4t/predict/README.md  \n  inflating: seamless_communication-main/scripts/m4t/predict/predict.py  \n extracting: seamless_communication-main/seamlessM4T.png  \n  inflating: seamless_communication-main/setup.py  \n   creating: seamless_communication-main/src/\n   creating: seamless_communication-main/src/seamless_communication/\n  inflating: seamless_communication-main/src/seamless_communication/__init__.py  \n   creating: seamless_communication-main/src/seamless_communication/assets/\n  inflating: seamless_communication-main/src/seamless_communication/assets/__init__.py  \n   creating: seamless_communication-main/src/seamless_communication/assets/cards/\n  inflating: seamless_communication-main/src/seamless_communication/assets/cards/seamlessM4T_large.yaml  \n  inflating: seamless_communication-main/src/seamless_communication/assets/cards/seamlessM4T_medium.yaml  \n  inflating: seamless_communication-main/src/seamless_communication/assets/cards/unity_nllb-100.yaml  \n  inflating: seamless_communication-main/src/seamless_communication/assets/cards/unity_nllb-200.yaml  \n  inflating: seamless_communication-main/src/seamless_communication/assets/cards/vocoder_36langs.yaml  \n  inflating: seamless_communication-main/src/seamless_communication/assets/download_manager.py  \n  inflating: seamless_communication-main/src/seamless_communication/assets/store.py  \n   creating: seamless_communication-main/src/seamless_communication/datasets/\n extracting: seamless_communication-main/src/seamless_communication/datasets/__init__.py  \n  inflating: seamless_communication-main/src/seamless_communication/datasets/datatypes.py  \n  inflating: seamless_communication-main/src/seamless_communication/datasets/huggingface.py  \n   creating: seamless_communication-main/src/seamless_communication/models/\n  inflating: seamless_communication-main/src/seamless_communication/models/__init__.py  \n   creating: seamless_communication-main/src/seamless_communication/models/inference/\n  inflating: seamless_communication-main/src/seamless_communication/models/inference/__init__.py  \n  inflating: seamless_communication-main/src/seamless_communication/models/inference/translator.py  \n   creating: seamless_communication-main/src/seamless_communication/models/unity/\n  inflating: seamless_communication-main/src/seamless_communication/models/unity/__init__.py  \n  inflating: seamless_communication-main/src/seamless_communication/models/unity/adaptor_block.py  \n  inflating: seamless_communication-main/src/seamless_communication/models/unity/builder.py  \n  inflating: seamless_communication-main/src/seamless_communication/models/unity/generator.py  \n  inflating: seamless_communication-main/src/seamless_communication/models/unity/loader.py  \n  inflating: seamless_communication-main/src/seamless_communication/models/unity/model.py  \n  inflating: seamless_communication-main/src/seamless_communication/models/unity/unit_tokenizer.py  \n   creating: seamless_communication-main/src/seamless_communication/models/vocoder/\n  inflating: seamless_communication-main/src/seamless_communication/models/vocoder/__init__.py  \n  inflating: seamless_communication-main/src/seamless_communication/models/vocoder/builder.py  \n  inflating: seamless_communication-main/src/seamless_communication/models/vocoder/codehifigan.py  \n  inflating: seamless_communication-main/src/seamless_communication/models/vocoder/hifigan.py  \n  inflating: seamless_communication-main/src/seamless_communication/models/vocoder/loader.py  \n  inflating: seamless_communication-main/src/seamless_communication/models/vocoder/vocoder.py  \n\n\nУстановим модель\n\n\nCode\n!cd seamless_communication-main && pip install .\n\n\nProcessing /content/seamless_communication-main\n  Preparing metadata (setup.py) ... done\nBuilding wheels for collected packages: seamless-communication\n  Building wheel for seamless-communication (setup.py) ... done\n  Created wheel for seamless-communication: filename=seamless_communication-0.1-py3-none-any.whl size=42637 sha256=5a85f0ddcf4ee0fae9974e4f3a23fd3ffa4161c7161483073f94f3c4cd53d0e5\n  Stored in directory: /root/.cache/pip/wheels/73/7d/af/7c699ab4b97c66df074478227d6613ea29b940c26811beec2c\nSuccessfully built seamless-communication\nInstalling collected packages: seamless-communication\nSuccessfully installed seamless-communication-0.1\n\n\nЗагрузим модель\n\n\nCode\nimport torch\nfrom seamless_communication.models.inference import Translator\n\n\n# Initialize a Translator object with a multitask model, vocoder on the GPU.\ntranslator = Translator(\"seamlessM4T_large\", vocoder_name_or_card=\"vocoder_36langs\", device=torch.device(\"cuda\"))\n\n\nDownloading the checkpoint of the model 'seamlessM4T_large'...\n100%|██████████| 10.7G/10.7G [01:42&lt;00:00, 111MB/s] \nDownloading the tokenizer of the model 'seamlessM4T_large'...\n100%|██████████| 4.93M/4.93M [00:00&lt;00:00, 27.2MB/s]\nDownloading the checkpoint of the model 'vocoder_36langs'...\n100%|██████████| 160M/160M [00:03&lt;00:00, 52.4MB/s]\n\n\nУстановим пакет torchaudio для работы с аудифайлом\n\n\nCode\n!pip install torchaudio\n\n\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.0.2+cu118)\nRequirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchaudio) (2.0.1+cu118)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;torchaudio) (3.12.2)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;torchaudio) (4.7.1)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;torchaudio) (1.12)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;torchaudio) (3.1)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;torchaudio) (3.1.2)\nRequirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-&gt;torchaudio) (2.0.0)\nRequirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-&gt;torch==2.0.1-&gt;torchaudio) (3.27.2)\nRequirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-&gt;torch==2.0.1-&gt;torchaudio) (16.0.6)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;torch==2.0.1-&gt;torchaudio) (2.1.3)\nRequirement already satisfied: mpmath&gt;=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-&gt;torch==2.0.1-&gt;torchaudio) (1.3.0)\n\n\n\n\nCode\nimport torchaudio\n\n\nВсе готово, и можно приступать к тестрованию\nText-to-speech translation\n\n\nCode\ntxt_orig=\"Machine learning (ML) is an umbrella term for solving problems for which development of algorithms by human programmers would be cost-prohibitive, and instead the problems are solved by helping machines 'discover' their 'own' algorithms, without needing to be explicitly told what to do by any human-developed algorithms. Recently, generative artificial neural networks have been able to surpass results of many previous approaches. Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.\"\n\n\n\n\nCode\n# T2ST\ntranslated_text, wav, sr = translator.predict(txt_orig, \"t2st\", tgt_lang=\"rus\", src_lang=\"eng\")\n\n# Save the translated audio generation.\ntorchaudio.save(\n   \"1.mp3\",\n    wav[0].cpu(),\n    sample_rate=sr,\n)\n\n\nSpeech-to-speech translation (S2ST)\nСкачаем ролик с сайта BBC\n\n\nCode\n!wget https://open.live.bbc.co.uk/mediaselector/6/redir/version/2.0/mediaset/audio-nondrm-download/proto/https/vpid/p0g764lc.mp3\n\n\n--2023-08-22 16:47:11--  https://open.live.bbc.co.uk/mediaselector/6/redir/version/2.0/mediaset/audio-nondrm-download/proto/https/vpid/p0g764lc.mp3\nResolving open.live.bbc.co.uk (open.live.bbc.co.uk)... 212.58.249.158, 212.58.244.79\nConnecting to open.live.bbc.co.uk (open.live.bbc.co.uk)|212.58.249.158|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://flex.acast.com/ak/mpg_mp3_med/modav/bUnknown-916773b6-c2bc-4a5c-b42e-664b06980323_p0g764lc_cUnknown_1692263409705.mp3?__gda__=1692744432_bca2578e0a54e90f98dc6661649c8296 [following]\n--2023-08-22 16:47:12--  https://flex.acast.com/ak/mpg_mp3_med/modav/bUnknown-916773b6-c2bc-4a5c-b42e-664b06980323_p0g764lc_cUnknown_1692263409705.mp3?__gda__=1692744432_bca2578e0a54e90f98dc6661649c8296\nResolving flex.acast.com (flex.acast.com)... 65.8.178.29, 65.8.178.19, 65.8.178.71, ...\nConnecting to flex.acast.com (flex.acast.com)|65.8.178.29|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://aod-pod-ww-live.akamaized.net/mpg_mp3_med/modav/bUnknown-916773b6-c2bc-4a5c-b42e-664b06980323_p0g764lc_cUnknown_1692263409705.mp3?__gda__=1692744432_bca2578e0a54e90f98dc6661649c8296 [following]\n--2023-08-22 16:47:12--  https://aod-pod-ww-live.akamaized.net/mpg_mp3_med/modav/bUnknown-916773b6-c2bc-4a5c-b42e-664b06980323_p0g764lc_cUnknown_1692263409705.mp3?__gda__=1692744432_bca2578e0a54e90f98dc6661649c8296\nResolving aod-pod-ww-live.akamaized.net (aod-pod-ww-live.akamaized.net)... 23.49.5.46, 23.49.5.20\nConnecting to aod-pod-ww-live.akamaized.net (aod-pod-ww-live.akamaized.net)|23.49.5.46|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 6001291 (5.7M) [audio/mpeg]\nSaving to: ‘p0g764lc.mp3’\n\np0g764lc.mp3        100%[===================&gt;]   5.72M  --.-KB/s    in 0.1s    \n\n2023-08-22 16:47:12 (58.7 MB/s) - ‘p0g764lc.mp3’ saved [6001291/6001291]\n\n\n\nУстановим библиотеку для работы с ffmpeg\n\n\nCode\n!pip install ffmpeg-python\n\n\nCollecting ffmpeg-python\n  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\nRequirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python) (0.18.3)\nInstalling collected packages: ffmpeg-python\nSuccessfully installed ffmpeg-python-0.2.0\n\n\nУдалим предыдущий файл wav\n\n\nCode\n!rm p0g764lc.wav\n\n\nОбрежем наш исходный файл, и сконвертируем его в формат wav\n\n\nCode\npath_to_orig_audio=\"p0g764lc.mp3\"\npath_to_input_audio=\"p0g764lc.wav\"\n\nimport ffmpeg\n(\n    ffmpeg\n    .input(path_to_orig_audio).audio.filter('atrim', duration=20)\n    .output(path_to_input_audio)\n    .run()\n)\n\n\n(None, None)\n\n\nПолучим перевод файла в текст и аудио\n\n\nCode\n# S2ST\n\ntranslated_text, wav, sr = translator.predict(path_to_input_audio, \"s2st\", tgt_lang=\"rus\",)\n\n\nВыведем текст на экран\n\n\nCode\nprint(translated_text)\n\n\nЗдравствуйте, это шестиминутный английский с BBC Learning English. Я Нил и я Бет. У вас когда-нибудь был ужасный босс, которому не платили достаточно или который просто устал делать ту же старую работу?\n\n\nИ сохраним аудио в файл\n\n\nCode\n# Save the translated audio generation.\ntorchaudio.save(\n    \"BBC.mp3\",\n    wav[0].cpu(),\n    sample_rate=sr,\n)\n\n\nText-to-text translation (T2TT)\nПолучим перевод текста в текст\n\n\nCode\n# T2TT\ntranslated_text, _, _ = translator.predict(txt_orig, \"t2tt\", tgt_lang=\"rus\", src_lang=\"eng\")\n\n\nи выведем текстовый перевод на экран\n\n\nCode\nprint(translated_text)\n\n\nМашинное обучение (ML) - это общий термин для решения проблем, для которых разработка алгоритмов человеческими программистами была бы недоступна, и вместо этого проблемы решаются, помогая машинам \"открыть\" свои \"собственные\" алгоритмы, без необходимости явного указания, что делать любыми разработанными человеком алгоритмами. В последнее время генеративные искусственные нейронные сети смогли превзойти результаты многих предыдущих подходов. Подходы машинного обучения были применены к большим языковым моделям, компьютерному зрению, распознаванию речи, фильтрации электронной почты, сельскому хозяйству и медицине, где разработка алгоритмов для выполнения необходимых задач слишком дорога.\n\n\nAutomatic speech recognition (ASR)\nПолучим транскрибцию аудио файла\n\n\nCode\n# ASR\n# This is equivalent to S2TT with `&lt;tgt_lang&gt;=&lt;src_lang&gt;`.\ntranscribed_text, _, _ = translator.predict(path_to_input_audio, \"asr\", \"eng\")\n\n\nи выведем ее на экран\n\n\nCode\nprint(transcribed_text)\n\n\nHello this is Six Minutes English from BBC Learning English I'm Neil and I'm Beth. Have you ever had a horrible boss not been paid enough or simply got tired of doing the same old-fashioned job?"
  },
  {
    "objectID": "posts/post-with-code/GrabData.html",
    "href": "posts/post-with-code/GrabData.html",
    "title": "Grab Wiki data/Получение данных с Wiki",
    "section": "",
    "text": "Для работы с текстом необходимо иметь некий массив текста достаточно большого размера. И таким источником является Wiki. Так что рассмотрим, как мы можем забрать эту информацию."
  },
  {
    "objectID": "posts/post-with-code/GrabData.html#parsing-xml",
    "href": "posts/post-with-code/GrabData.html#parsing-xml",
    "title": "Grab Wiki data/Получение данных с Wiki",
    "section": "Parsing XML",
    "text": "Parsing XML\n\n\nCode\nimport xml.sax\n\nclass WikiXmlHandler(xml.sax.handler.ContentHandler):\n    \"\"\"Content handler for Wiki XML data using SAX\"\"\"\n    def __init__(self):\n        xml.sax.handler.ContentHandler.__init__(self)\n        self._buffer = None\n        self._values = {}\n        self._current_tag = None\n        self._pages = []\n\n    def characters(self, content):\n        \"\"\"Characters between opening and closing tags\"\"\"\n        if self._current_tag:\n            self._buffer.append(content)\n\n    def startElement(self, name, attrs):\n        \"\"\"Opening tag of element\"\"\"\n        if name in ('title', 'text', 'timestamp'):\n            self._current_tag = name\n            self._buffer = []\n            #print(f'open tag {name} \\n')\n\n    def endElement(self, name):\n        \"\"\"Closing tag of element\"\"\"\n        if name == self._current_tag:\n            self._values[name] = ' '.join(self._buffer)\n\n        if name == 'page':\n            self._pages.append((self._values['title'], self._values['text']))\n        #print(f'close tag {name} \\n')"
  },
  {
    "objectID": "posts/post-with-code/GrabData.html#parsing-articles",
    "href": "posts/post-with-code/GrabData.html#parsing-articles",
    "title": "Grab Wiki data/Получение данных с Wiki",
    "section": "Parsing Articles",
    "text": "Parsing Articles\n\n\nCode\nimport subprocess\n\n\n\n\nCode\n# Object for handling xml\nhandler = WikiXmlHandler()\n\n# Parsing object\nparser = xml.sax.make_parser()\nparser.setContentHandler(handler)\n\nfor i, line in enumerate(subprocess.Popen(['bzcat'], \n                         stdin = open(data_path), \n                         stdout = subprocess.PIPE).stdout):\n    parser.feed(line)\n    \n    # Stop when 50 articles have been found\n#    if len(handler._pages) &gt; 50:\n#        break\n\n\n\n\nCode\ndef parseArticles(data_path):\n  # Object for handling xml\n  handler = WikiXmlHandler()\n\n  # Parsing object\n  parser = xml.sax.make_parser()\n  parser.setContentHandler(handler)\n\n  for i, line in enumerate(subprocess.Popen(['bzcat'], \n                         stdin = open(data_path), \n                         stdout = subprocess.PIPE).stdout):\n    parser.feed(line)\n  \n  return handler\n\n\n\n\nCode\nimport mwparserfromhell \n\n\n\n\nCode\ndef process_article(title, text, timestamp):\n    \"\"\"Process a wikipedia article\"\"\"\n    # Create a parsing object\n    wikicode = mwparserfromhell.parse(text)\n    return (title, wikicode.strip_code().strip(), timestamp)"
  },
  {
    "objectID": "posts/post-with-code/GrabData.html#process-bulk-pages",
    "href": "posts/post-with-code/GrabData.html#process-bulk-pages",
    "title": "Grab Wiki data/Получение данных с Wiki",
    "section": "Process Bulk Pages",
    "text": "Process Bulk Pages\nБудем сохранять распарсеные файлы в один большой архив, с которым проще иметь дело, чем с кучей мелких файлов\n\n\nCode\nimport zipfile\n\n\n\n\nCode\nstopTitle=('Файл:','Категория:','Википедия:','MediaWiki:','Шаблон:','Портал:','Проект:')\nstopText=('перенаправление','REDIRECT','redirect','Redirect')\n\n\nне предпочтительный способ выгрузки\n\n\nCode\ndef pocessBundleZip(pages, archive_name = 'archive.zip'):\n  \"\"\"Save exctacted wikipedia article in zip file\"\"\"\n  with zipfile.ZipFile(archive_name, mode=\"a\") as archive:\n    for page_no,page in enumerate(pages):\n      title, wikitext,_ = process_article(pages[page_no][0],pages[page_no][1],None)\n      if not title.startswith(stopTitle):\n        if not wikitext.startswith(stopText):\n          fname = title.replace(', ',' ').replace('/','-')+'.txt'\n          text = wikitext.replace('&lt; ref &gt;  &lt; /ref &gt;','').replace('&lt; ref &gt;','(').replace('&lt; /ref &gt;',')').replace('  .','.').replace('  ,',',').replace(' .','.')\n          with archive.open(fname, \"w\") as new_file:\n            new_file.write(bytes(text,'utf-8'))\n\n\nпредпочтительный способ выгрузки\n\n\nCode\ndef pocessBundleBZ2(pages, archive_name = 'archive.bz2'):\n  \"\"\"Save exctacted wikipedia article in bzip file\"\"\"\n  with zipfile.ZipFile(archive_name, mode=\"a\", compression=zipfile.ZIP_BZIP2) as archive:\n    for page_no,page in enumerate(pages):\n      title, wikitext,_ = process_article(pages[page_no][0],pages[page_no][1],None)\n      if not title.startswith(stopTitle):\n        if not wikitext.startswith(stopText):\n          fname = title.replace(', ',' ').replace('/','-')+'.txt'\n          text = wikitext.replace('&lt; ref &gt;  &lt; /ref &gt;','').replace('&lt; ref &gt;','(').replace('&lt; /ref &gt;',')').replace('  .','.').replace('  ,',',').replace(' .','.')\n          with archive.open(fname, \"w\") as new_file:\n            new_file.write(bytes(text,'utf-8'))\n\n\n\n\nCode\n# Process Articles\nhandler = parseArticles(data_path)\n# Get dump name\nfilename = os.path.splitext(os.path.basename(data_path))[0]\n# Set Archive name\narchName = keras_home + archives + filename + '.bz2'\n# Save processed articles in archive\npocessBundleBZ2(handler._pages, archName)\n\n\n\n\nCode\n# Check created file\nwith zipfile.ZipFile(archName, mode=\"r\",compression=zipfile.ZIP_BZIP2) as archive:\n  #archive.printdir()\n  print(\"------\")\n  text = archive.read(\"Джимасар.txt\").decode(encoding=\"utf-8\")\n\nprint(text)\n\n\n\n\nCode\nfor page_no,page in enumerate(pages):\n      title, wikitext,_ = process_article(pages[page_no][0],pages[page_no][1],None)\n      print(f'')"
  },
  {
    "objectID": "posts/post-with-code/Demo of SberGPT3.html",
    "href": "posts/post-with-code/Demo of SberGPT3.html",
    "title": "Пример работы с GPT3 от Сбербанка",
    "section": "",
    "text": "Установим transformers\n\n\nCode\n!pip install transformers\n\n\nCollecting transformers\n  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.4/7.4 MB 55.8 MB/s eta 0:00:00\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\nCollecting huggingface-hub&lt;1.0,&gt;=0.14.1 (from transformers)\n  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 26.0 MB/s eta 0:00:00\nRequirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\nRequirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\nCollecting tokenizers!=0.11.3,&lt;0.14,&gt;=0.11.1 (from transformers)\n  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 107.3 MB/s eta 0:00:00\nCollecting safetensors&gt;=0.3.1 (from transformers)\n  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 81.0 MB/s eta 0:00:00\nRequirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&lt;1.0,&gt;=0.14.1-&gt;transformers) (2023.6.0)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&lt;1.0,&gt;=0.14.1-&gt;transformers) (4.7.1)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (3.2.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (2.0.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (2023.7.22)\nInstalling collected packages: tokenizers, safetensors, huggingface-hub, transformers\nSuccessfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n\n\nДалее установим необходимые модули и загрузим сам модуль\n\n\nCode\nimport torch\nfrom transformers import AutoTokenizer, AutoModel, AutoModelWithLMHead, AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoModelForMaskedLM\n\nmodel_name_or_path = \"sberbank-ai/rugpt3large_based_on_gpt2\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path)\nmodel.eval()\nmodel.to('cuda')\n\n\n\n\n\n\n\n\n\n\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\n\n\n\n\nGPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 1536)\n    (wpe): Embedding(2048, 1536)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-23): 24 x GPT2Block(\n        (ln_1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=1536, out_features=50257, bias=False)\n)\n\n\nСоздадим обертку generate для удобства работы\n\n\nCode\ndef generate(text, max_length):\n  # encode context the generation is conditioned on\n  input_ids = tokenizer.encode(text, return_tensors='pt')\n  input_ids = input_ids.to('cuda')\n\n  # generate text until the output length (which includes the context length) reaches max_length\n  greedy_output = model.generate(input_ids, max_length=max_length,no_repeat_ngram_size=5,\n    repetition_penalty=2.)\n  return tokenizer.decode(greedy_output[0], skip_special_tokens=True).split(\"&lt;s&gt;\")[0] # очищаем от мусора в ответе\n\n\nИ можем проверить работу загруженного модуля\n\n\nCode\n# @title Работа с моделью\n\n\nstart_text = \"Кто такой Пушкин? \" # @param {type:\"string\"}\ngen_lenth = 800 # @param {type:\"integer\"}\n\ntext = generate(start_text,\n         gen_lenth)\nprint(text)\n\n\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\nКто такой Пушкин? \n- Это я, - сказал он."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/post-with-code/hello.html",
    "href": "posts/post-with-code/hello.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis\n\n\n\n\n\n\nCode\n#hide\n!quarto render hello.ipynb --to html"
  },
  {
    "objectID": "posts/post-with-code/hello.html#polar-axis",
    "href": "posts/post-with-code/hello.html#polar-axis",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis\n\n\n\n\n\n\nCode\n#hide\n!quarto render hello.ipynb --to html"
  },
  {
    "objectID": "posts/post-with-code/style transfers by TF.html",
    "href": "posts/post-with-code/style transfers by TF.html",
    "title": "Пример реализации переноса стиля",
    "section": "",
    "text": "Основано на коде модели magenta и публикации:\nExploring the structure of a real-time, arbitrary neural artistic stylization network. Golnaz Ghiasi, Honglak Lee, Manjunath Kudlur, Vincent Dumoulin, Jonathon Shlens, Proceedings of the British Machine Vision Conference (BMVC), 2017."
  },
  {
    "objectID": "posts/post-with-code/style transfers by TF.html#настройка",
    "href": "posts/post-with-code/style transfers by TF.html#настройка",
    "title": "Пример реализации переноса стиля",
    "section": "Настройка",
    "text": "Настройка\nИмпортритуем TF2 и нужные модули\n\n\nCode\nimport functools\nimport os\n\nfrom matplotlib import gridspec\nimport matplotlib.pylab as plt\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nprint(\"TF Version: \", tf.__version__)\nprint(\"TF Hub version: \", hub.__version__)\nprint(\"Eager mode enabled: \", tf.executing_eagerly())\nprint(\"GPU available: \", tf.config.list_physical_devices('GPU'))\n\n\nTF Version:  2.12.0\nTF Hub version:  0.14.0\nEager mode enabled:  True\nGPU available:  []\n\n\n\n\nCode\n# @title Определим функции загрузки изображений и отображения  { display-mode: \"form\" }\n\ndef crop_center(image):\n  \"\"\"Returns a cropped square image.\"\"\"\n  shape = image.shape\n  new_shape = min(shape[1], shape[2])\n  offset_y = max(shape[1] - shape[2], 0) // 2\n  offset_x = max(shape[2] - shape[1], 0) // 2\n  image = tf.image.crop_to_bounding_box(\n      image, offset_y, offset_x, new_shape, new_shape)\n  return image\n\n@functools.lru_cache(maxsize=None)\ndef load_image(image_url, image_size=(256, 256), preserve_aspect_ratio=True):\n  \"\"\"Loads and preprocesses images.\"\"\"\n  # Cache image file locally.\n  image_path = tf.keras.utils.get_file(os.path.basename(image_url)[-128:], image_url)\n  # Load and convert to float32 numpy array, add batch dimension, and normalize to range [0, 1].\n  img = tf.io.decode_image(\n      tf.io.read_file(image_path),\n      channels=3, dtype=tf.float32)[tf.newaxis, ...]\n  img = crop_center(img)\n  img = tf.image.resize(img, image_size, preserve_aspect_ratio=True)\n  return img\n\ndef show_n(images, titles=('',)):\n  n = len(images)\n  image_sizes = [image.shape[1] for image in images]\n  w = (image_sizes[0] * 6) // 320\n  plt.figure(figsize=(w * n, w))\n  gs = gridspec.GridSpec(1, n, width_ratios=image_sizes)\n  for i in range(n):\n    plt.subplot(gs[i])\n    plt.imshow(images[i][0], aspect='equal')\n    plt.axis('off')\n    plt.title(titles[i] if len(titles) &gt; i else '')\n  plt.show()\n\n\nЗагрузим изображения, и посмотрим их\n\n\nCode\n# @title Загрузим образцы изображений  { display-mode: \"form\" }\n\ncontent_image_url = 'https://biblioclub.ru/services/fks.php?fks_action=get_file&fks_id=31762774&fks_flag=2'  # @param {type:\"string\"}\nstyle_image_url = 'https://upload.wikimedia.org/wikipedia/commons/3/37/Derkovits_Gyula_Talig%C3%A1s_1920.jpg'  # @param {type:\"string\"}\noutput_image_size = 384  # @param {type:\"integer\"}\n\n# The content image size can be arbitrary.\ncontent_img_size = (output_image_size, output_image_size)\n# The style prediction model was trained with image size 256 and it's the\n# recommended image size for the style image (though, other sizes work as\n# well but will lead to different results).\nstyle_img_size = (256, 256)  # Recommended to keep it at 256.\n\ncontent_image = load_image(content_image_url, content_img_size)\nstyle_image = load_image(style_image_url, style_img_size)\nstyle_image = tf.nn.avg_pool(style_image, ksize=[3,3], strides=[1,1], padding='SAME')\nshow_n([content_image, style_image], ['Content image', 'Style image'])\n\n\nDownloading data from https://biblioclub.ru/services/fks.php?fks_action=get_file&fks_id=31762774&fks_flag=2\n34061/34061 [==============================] - 0s 5us/step\nDownloading data from https://upload.wikimedia.org/wikipedia/commons/3/37/Derkovits_Gyula_Talig%C3%A1s_1920.jpg\n40620/40620 [==============================] - 0s 0us/step"
  },
  {
    "objectID": "posts/post-with-code/style transfers by TF.html#загрузка-модуля-tf-hub",
    "href": "posts/post-with-code/style transfers by TF.html#загрузка-модуля-tf-hub",
    "title": "Пример реализации переноса стиля",
    "section": "Загрузка модуля TF Hub",
    "text": "Загрузка модуля TF Hub\n\n\nCode\n# Load TF Hub module.\n\nhub_handle = 'https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2'\nhub_module = hub.load(hub_handle)\n\n\nСигнатура этого модуля для стилизации изображений следующая:\noutputs = hub_module(content_image, style_image)\nstylized_image = outputs[0]\nГде content_image, style_image, stylized_image являются 4-D Тензорами с размерностью [batch_size, image_height, image_width, 3].\nВ этом примере мы подадим только одно тзображение, поэтому размерность бача 1, но этот модуль можно использовать и для обработки нескольких изображений за раз.\nВходные и выходные значения изображения должны быть в диапазоне [0, 1].\nРазмерность и первичного изображения и стилевого не обязательно должны совпадать. Результирующее изображение будет иметь размерность первичного изображение."
  },
  {
    "objectID": "posts/post-with-code/style transfers by TF.html#демонстрация-стилизации",
    "href": "posts/post-with-code/style transfers by TF.html#демонстрация-стилизации",
    "title": "Пример реализации переноса стиля",
    "section": "Демонстрация стилизации",
    "text": "Демонстрация стилизации\n\n\nCode\n# Stylize content image with given style image.\n# This is pretty fast within a few milliseconds on a GPU.\n\noutputs = hub_module(tf.constant(content_image), tf.constant(style_image))\nstylized_image = outputs[0]\n\n\n\n\nCode\n# Visualize input images and the generated stylized image.\n\nshow_n([content_image, style_image, stylized_image], titles=['Original content image', 'Style image', 'Stylized image'])"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Это мой первый пост в Quarto blog. Welcome!\n\nВ данном блоге буду резмещать посты касающиеся машинного обучения и связанных с ним вопросов"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ml-blog",
    "section": "",
    "text": "Пример работы с seamless-m4t-large от Мета\n\n\n\n\n\n\n\ncode\n\n\ntext\n\n\naudio\n\n\n\n\n\n\n\n\n\n\n\nAug 22, 2023\n\n\nVladimir Orlenko\n\n\n\n\n\n\n  \n\n\n\n\nПример работы с GPT3 от Сбербанка\n\n\n\n\n\n\n\ncode\n\n\ntext\n\n\n\n\n\n\n\n\n\n\n\nAug 22, 2023\n\n\nVladimir Orlenko\n\n\n\n\n\n\n  \n\n\n\n\nГенерация изображений с помощью модели SDXL (sd_xl_base_1.0)\n\n\n\n\n\n\n\ncode\n\n\nimage\n\n\n\n\n\n\n\n\n\n\n\nAug 21, 2023\n\n\nVladimir Orlenko\n\n\n\n\n\n\n  \n\n\n\n\nНайдено применение фотореализму UE5\n\n\n\n\n\n\n\nnews\n\n\ndataset\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2023\n\n\nVladimir Orlenko\n\n\n\n\n\n\n  \n\n\n\n\nПример реализации переноса стиля\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nAug 12, 2023\n\n\nVladimir Orlenko\n\n\n\n\n\n\n  \n\n\n\n\nAналог Github Copilot на Stability AI\n\n\n\n\n\n\n\ncode\n\n\ntext\n\n\n\n\n\n\n\n\n\n\n\nAug 11, 2023\n\n\nVladimir Orlenko\n\n\n\n\n\n\n  \n\n\n\n\nGrab Wiki data/Получение данных с Wiki\n\n\n\n\n\n\n\ncode\n\n\ndata\n\n\ntext\n\n\n\n\n\n\n\n\n\n\n\nJul 31, 2023\n\n\nVladimir Orlenko\n\n\n\n\n\n\n  \n\n\n\n\nCreate Mario Levels\n\n\n\n\n\n\n\ncode\n\n\ntext\n\n\n\n\n\n\n\n\n\n\n\nJul 29, 2023\n\n\nVladimir Orlenko\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJul 29, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nQuarto Basics\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJul 29, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJul 26, 2023\n\n\nVladimir Orlenko\n\n\n\n\n\n\nNo matching items"
  }
]